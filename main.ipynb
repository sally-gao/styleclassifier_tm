{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import itertools\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import csv\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_corpus(path, replace_dict, min_length=10):\n",
    "    \"\"\"\n",
    "    Removes source-specific artifacts from documents.\n",
    "    \n",
    "    :param path: filepath containing JSON files for each document\n",
    "    :param replace_dict: dictionary containing regex matching strings and strings to replace them with\n",
    "    :param min_length: minimum document length\n",
    "    \n",
    "    :returns: list of strings, each containing document content\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = []\n",
    "    #files = os.listdir(path)\n",
    "    json_files = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n",
    "\n",
    "    for file in json_files:\n",
    "        #file = '\\\\' + file\n",
    "        content = json.load(open(path + file))['content']\n",
    "        \n",
    "        # replace regex strings\n",
    "        for key, value in replace_dict.items():\n",
    "            content = re.sub(key, value, content)\n",
    "        \n",
    "        # remove small documents\n",
    "        if len(content) >= min_length:\n",
    "            docs.append(content)\n",
    "        \n",
    "    return(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "economist_path = 'data_updated/economist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "economist_dict = {}\n",
    "\n",
    "# artifacts on accented letters\n",
    "economist_dict['AaAaAeA '] = 'i'\n",
    "economist_dict['AaAaAeAo'] = 'c'\n",
    "economist_dict['AaAaAeAc'] = 'a'\n",
    "economist_dict['AaAaAeA~'] = 'n'\n",
    "economist_dict['AaAaAeA@|AaAaAeA\\?|AaAaAeA{|AaAaAe'] = 'e'\n",
    "\n",
    "\n",
    "\n",
    "# numbers\n",
    "economist_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "\n",
    "# Go online artifacts\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "economist_dict['<p>Go online ([^<]*)</p>|<p>([^<]*)([^.?!\"]){1}</p>'] = ''\n",
    "\n",
    "# end of paragraph tags\n",
    "economist_dict['</p>'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "economist_docs = process_corpus(economist_path, economist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104\n"
     ]
    }
   ],
   "source": [
    "print(len(economist_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "economist_paragraphs = []\n",
    "for doc in economist_docs:\n",
    "    economist_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "economist_paragraphs = [doc.strip() for doc in economist_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11198"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(economist_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wired_path = 'data_updated/wired/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wired_dict = {}\n",
    "\n",
    "# numbers\n",
    "wired_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "# author/subject descriptions at end of article\n",
    "# paragraph symbols\n",
    "wired_dict['<p>([^<]*)([^.?!\"]){1}</p>|<p>([^<]*)([A-Z]+) \\(@(.*)|Â¶'] = ''\n",
    "\n",
    "#email addresses\n",
    "wired_dict['[\\w\\.-]+@[\\w\\.-]+']=''\n",
    "\n",
    "# end of paragraph tags\n",
    "wired_dict['</p>'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wired_docs = process_corpus(wired_path, wired_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296\n"
     ]
    }
   ],
   "source": [
    "print(len(wired_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wired_paragraphs = []\n",
    "\n",
    "for doc in wired_docs:\n",
    "    wired_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "wired_paragraphs = [doc.strip() for doc in wired_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17142"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wired_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Yorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newyorker_path = 'data_updated/newyorker/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newyorker_dict = {}\n",
    "\n",
    "# artifacts on accented letters\n",
    "newyorker_dict['AaAaAeA '] = 'i'\n",
    "newyorker_dict['AaAaAeAo'] = 'c'\n",
    "newyorker_dict['AaAaAeAc'] = 'a'\n",
    "newyorker_dict['AaAaAeA~'] = 'n'\n",
    "newyorker_dict['AaAaAeA@|AaAaAeA\\?|AaAaAeA{|AaAaAe'] = 'e'\n",
    "\n",
    "# numbers\n",
    "newyorker_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "# bylines\n",
    "newyorker_dict['<p>([^<]*)([^.?!\"]){1}</p>|<p>Byline([^<]*)</p>'] = ''\n",
    "\n",
    "# end of paragraph tags\n",
    "newyorker_dict['</p>'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newyorker_docs = process_corpus(newyorker_path, newyorker_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807\n"
     ]
    }
   ],
   "source": [
    "print(len(newyorker_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newyorker_paragraphs = []\n",
    "for doc in newyorker_docs:\n",
    "    newyorker_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "newyorker_paragraphs = [doc.strip() for doc in newyorker_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19030"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newyorker_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ew_path = 'data_updated/ew/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ew_dict = {}\n",
    "\n",
    "# artifacts on accented letters\n",
    "ew_dict['AaAaAeA '] = 'i'\n",
    "ew_dict['AaAaAeAo'] = 'c'\n",
    "ew_dict['AaAaAeAc'] = 'a'\n",
    "ew_dict['AaAaAeA~'] = 'n'\n",
    "ew_dict['AaAaAeA@|AaAaAeA\\?|AaAaAeA{|AaAaAe'] = 'e'\n",
    "\n",
    "# numbers\n",
    "ew_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "#email addresses\n",
    "ew_dict['[\\w\\.-]+@[\\w\\.-]+']=''\n",
    "\n",
    "# Go online artifacts\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "#ew_dict['<p>Go online ([^<]*)</p>|<p>([^<]*)([^.?!+\"]){1}</p>'] = ''\n",
    "\n",
    "#bullet points\n",
    "ew_dict['\\xc2\\xb7']=''\n",
    "\n",
    "# end of paragraph tags\n",
    "ew_dict['</p>'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ew_docs = process_corpus(ew_path, ew_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2190\n"
     ]
    }
   ],
   "source": [
    "print(len(ew_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ew_paragraphs = []\n",
    "\n",
    "for doc in ew_docs:\n",
    "    ew_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "ew_paragraphs = [doc.strip() for doc in ew_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_ratings(graf):\n",
    "    \n",
    "    \"\"\"Removes ratings such as \"B+\", \"A\", \"C-\" from the end of a paragraph.\"\"\"\n",
    "    \n",
    "    # Check if the last character in the last word is a sentence-ending character ('.', '!' and so on)\n",
    "    if not any(x == graf.split()[-1][-1] for x in ['.', '!', '?', '\\\"', ')', 'â', 'â¦']):\n",
    "        \n",
    "        # If not, return paragraph with the last word removed\n",
    "        return graf[0:-len(graf.split()[-1])].strip()\n",
    "    \n",
    "    else:\n",
    "        return graf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ew_paragraphs = [remove_ratings(p) for p in ew_paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17536\n"
     ]
    }
   ],
   "source": [
    "print(len(ew_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragraphs = economist_paragraphs + wired_paragraphs + newyorker_paragraphs+ew_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = list(itertools.repeat('economist', len(economist_paragraphs)))\n",
    "sources += list(itertools.repeat('wired', len(wired_paragraphs)))\n",
    "sources += list(itertools.repeat('newyorker', len(newyorker_paragraphs)))\n",
    "sources +=list(itertools.repeat('ew',len(ew_paragraphs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'content':paragraphs, 'source':sources}\n",
    "final_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64906"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOWN the Euphrates river, halfway between Deir...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Never have America and its allies had such a h...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But like their Parthian forebears, Iran and it...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iran's gains are even more striking elsewhere....</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Farther south, America's hopes of stemming Ira...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content     source\n",
       "0  DOWN the Euphrates river, halfway between Deir...  economist\n",
       "1  Never have America and its allies had such a h...  economist\n",
       "2  But like their Parthian forebears, Iran and it...  economist\n",
       "3  Iran's gains are even more striking elsewhere....  economist\n",
       "4  Farther south, America's hopes of stemming Ira...  economist"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final_df.to_csv('pre_pre_processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeHTMLTags(x):\n",
    "    content = re.sub(\"(?i)<\\/?\\w+((\\s+\\w+(\\s*=\\s*(?:\\\".*?\\\"|'.*?'|[^'\\\">\\s]+))?)+\\s*|\\s*)\\/?>\", '', x)\n",
    "    #content1 = re.sub(\"\\\\b\\\\x94\\\\b\", ' ', content)\n",
    "    \n",
    "    return(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df['Content_Preprocessed']=final_df['content'].apply(lambda x:removeHTMLTags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    DOWN the Euphrates river, halfway between Deir...\n",
       "1    Never have America and its allies had such a h...\n",
       "2    But like their Parthian forebears, Iran and it...\n",
       "3    Iran's gains are even more striking elsewhere....\n",
       "4    Farther south, America's hopes of stemming Ira...\n",
       "Name: Content_Preprocessed, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['Content_Preprocessed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter out paragraphs with 20 words or less\n",
    "\n",
    "final_df_filtered = final_df[final_df['Content_Preprocessed'].apply(lambda x:len(x.split()))>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55136"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence-Level Preprocessing Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "final_df_filtered['readability_f'] = final_df_filtered['Content_Preprocessed'].apply(textstat.flesch_reading_ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "final_df_filtered['readability_SMOG']=final_df_filtered['Content_Preprocessed'].apply(textstat.smog_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    55136.000000\n",
       "mean        62.799754\n",
       "std         14.746275\n",
       "min        -68.600000\n",
       "25%         53.380000\n",
       "50%         63.190000\n",
       "75%         72.760000\n",
       "max        116.350000\n",
       "Name: readability_f, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_filtered['readability_f'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "      <th>Content_Preprocessed</th>\n",
       "      <th>readability_f</th>\n",
       "      <th>readability_SMOG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>The exhibition proceeds broadly chronologicall...</td>\n",
       "      <td>economist</td>\n",
       "      <td>The exhibition proceeds broadly chronologicall...</td>\n",
       "      <td>-46.27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7844</th>\n",
       "      <td>Westron wynde, when wilt thou blow, the small ...</td>\n",
       "      <td>economist</td>\n",
       "      <td>Westron wynde, when wilt thou blow, the small ...</td>\n",
       "      <td>100.58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8479</th>\n",
       "      <td>Vladimir frowns and thinks. And then he clicks...</td>\n",
       "      <td>economist</td>\n",
       "      <td>Vladimir frowns and thinks. And then he clicks...</td>\n",
       "      <td>116.35</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11432</th>\n",
       "      <td>The raw components of the sneakers being produ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>The raw components of the sneakers being produ...</td>\n",
       "      <td>-7.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11447</th>\n",
       "      <td>#whatjayzsaidtosolange; coming within inches o...</td>\n",
       "      <td>wired</td>\n",
       "      <td>#whatjayzsaidtosolange; coming within inches o...</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11655</th>\n",
       "      <td>THE PROOF Hawaii and Stephen F. Austin, two te...</td>\n",
       "      <td>wired</td>\n",
       "      <td>THE PROOF Hawaii and Stephen F. Austin, two te...</td>\n",
       "      <td>102.61</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12279</th>\n",
       "      <td>Which reminds me. I've got a great idea for a ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Which reminds me. I've got a great idea for a ...</td>\n",
       "      <td>111.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12679</th>\n",
       "      <td>\"Your first place is home. Your second is work...</td>\n",
       "      <td>wired</td>\n",
       "      <td>\"Your first place is home. Your second is work...</td>\n",
       "      <td>106.37</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12953</th>\n",
       "      <td>Do not read this in a house. Do not read this ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Do not read this in a house. Do not read this ...</td>\n",
       "      <td>115.13</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13598</th>\n",
       "      <td>But he felt that he had no choice. Two months ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>But he felt that he had no choice. Two months ...</td>\n",
       "      <td>101.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13642</th>\n",
       "      <td>By June NUM it was a circle. No one takes full...</td>\n",
       "      <td>wired</td>\n",
       "      <td>By June NUM it was a circle. No one takes full...</td>\n",
       "      <td>103.32</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13698</th>\n",
       "      <td>If those million-eyed, fast-moving, deep-seate...</td>\n",
       "      <td>wired</td>\n",
       "      <td>If those million-eyed, fast-moving, deep-seate...</td>\n",
       "      <td>-2.97</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13699</th>\n",
       "      <td>But if the world's governments continue to ins...</td>\n",
       "      <td>wired</td>\n",
       "      <td>But if the world's governments continue to ins...</td>\n",
       "      <td>-35.11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14015</th>\n",
       "      <td>I ask Ferren how he'll keep a 4-year-old from ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>I ask Ferren how he'll keep a 4-year-old from ...</td>\n",
       "      <td>100.78</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14054</th>\n",
       "      <td>AND THE WINNER IS: Mozart. Sure, the dubstep k...</td>\n",
       "      <td>wired</td>\n",
       "      <td>AND THE WINNER IS: Mozart. Sure, the dubstep k...</td>\n",
       "      <td>101.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14317</th>\n",
       "      <td>The man, the myth, the legend: Bill Murray; Ju...</td>\n",
       "      <td>wired</td>\n",
       "      <td>The man, the myth, the legend: Bill Murray; Ju...</td>\n",
       "      <td>-11.42</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14382</th>\n",
       "      <td>We're bothered by AIs that try to sound human....</td>\n",
       "      <td>wired</td>\n",
       "      <td>We're bothered by AIs that try to sound human....</td>\n",
       "      <td>102.61</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14529</th>\n",
       "      <td>Advil; Aleve; Diet Coke; Papalote salsa; Comfy...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Advil; Aleve; Diet Coke; Papalote salsa; Comfy...</td>\n",
       "      <td>-68.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14780</th>\n",
       "      <td>Now here you are, on the cusp of carrying that...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Now here you are, on the cusp of carrying that...</td>\n",
       "      <td>-19.54</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15183</th>\n",
       "      <td>Land quarries and riverbanks were once the mai...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Land quarries and riverbanks were once the mai...</td>\n",
       "      <td>102.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15265</th>\n",
       "      <td>Who gets the bigger room? Who gets to name the...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Who gets the bigger room? Who gets to name the...</td>\n",
       "      <td>103.42</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15302</th>\n",
       "      <td>\"But I couldn't sell it at a gallery,\" I say. ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>\"But I couldn't sell it at a gallery,\" I say. ...</td>\n",
       "      <td>100.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15511</th>\n",
       "      <td>Buying a grill used to be simple: You went to ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Buying a grill used to be simple: You went to ...</td>\n",
       "      <td>101.60</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16081</th>\n",
       "      <td>Anaya stumbled back from the truck's cab, livi...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Anaya stumbled back from the truck's cab, livi...</td>\n",
       "      <td>106.37</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16203</th>\n",
       "      <td>We see the middle-aged man crouching in pain, ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>We see the middle-aged man crouching in pain, ...</td>\n",
       "      <td>101.60</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16571</th>\n",
       "      <td>âThis was my gift to you,â Kane shot back test...</td>\n",
       "      <td>wired</td>\n",
       "      <td>âThis was my gift to you,â Kane shot back test...</td>\n",
       "      <td>100.58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16590</th>\n",
       "      <td>Nestor was up more than $NUM. The Game King ri...</td>\n",
       "      <td>wired</td>\n",
       "      <td>Nestor was up more than $NUM. The Game King ri...</td>\n",
       "      <td>103.12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>And so this issue of WIRED brings you NUM page...</td>\n",
       "      <td>wired</td>\n",
       "      <td>And so this issue of WIRED brings you NUM page...</td>\n",
       "      <td>101.29</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16715</th>\n",
       "      <td>FIRST COMPUTER: I got this old black Dell lapt...</td>\n",
       "      <td>wired</td>\n",
       "      <td>FIRST COMPUTER: I got this old black Dell lapt...</td>\n",
       "      <td>100.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16778</th>\n",
       "      <td>[5] \"The water jug helps me track how much wat...</td>\n",
       "      <td>wired</td>\n",
       "      <td>[5] \"The water jug helps me track how much wat...</td>\n",
       "      <td>101.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60917</th>\n",
       "      <td>I did, because I'm a shoplifter [laughs]. Ther...</td>\n",
       "      <td>ew</td>\n",
       "      <td>I did, because I'm a shoplifter [laughs]. Ther...</td>\n",
       "      <td>102.61</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61069</th>\n",
       "      <td>AMONG THE OTHER GRAND ENDEAVORS SURE TO HAVE p...</td>\n",
       "      <td>ew</td>\n",
       "      <td>AMONG THE OTHER GRAND ENDEAVORS SURE TO HAVE p...</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61139</th>\n",
       "      <td>\"Sometimes\" by Britney Spears. [Laughs] It hap...</td>\n",
       "      <td>ew</td>\n",
       "      <td>\"Sometimes\" by Britney Spears. [Laughs] It hap...</td>\n",
       "      <td>102.61</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61217</th>\n",
       "      <td>I saw over NUM young men. Those were videotape...</td>\n",
       "      <td>ew</td>\n",
       "      <td>I saw over NUM young men. Those were videotape...</td>\n",
       "      <td>103.42</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61238</th>\n",
       "      <td>While we were shooting School Ties, we all wen...</td>\n",
       "      <td>ew</td>\n",
       "      <td>While we were shooting School Ties, we all wen...</td>\n",
       "      <td>105.35</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61241</th>\n",
       "      <td>We filmed the tribunal scene all day long. Par...</td>\n",
       "      <td>ew</td>\n",
       "      <td>We filmed the tribunal scene all day long. Par...</td>\n",
       "      <td>100.17</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61354</th>\n",
       "      <td>Cloris came in like a whirlwind, all over the ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>Cloris came in like a whirlwind, all over the ...</td>\n",
       "      <td>100.78</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61359</th>\n",
       "      <td>I was concerned that if I did a nude scene it ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>I was concerned that if I did a nude scene it ...</td>\n",
       "      <td>108.23</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61367</th>\n",
       "      <td>Ben Johnson, man. He was so great. I got to dr...</td>\n",
       "      <td>ew</td>\n",
       "      <td>Ben Johnson, man. He was so great. I got to dr...</td>\n",
       "      <td>106.16</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61373</th>\n",
       "      <td>I was mad that Peter only let me do that scene...</td>\n",
       "      <td>ew</td>\n",
       "      <td>I was mad that Peter only let me do that scene...</td>\n",
       "      <td>103.83</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61381</th>\n",
       "      <td>I didn't expect it at all. I got a dress and i...</td>\n",
       "      <td>ew</td>\n",
       "      <td>I didn't expect it at all. I got a dress and i...</td>\n",
       "      <td>103.63</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61851</th>\n",
       "      <td>\"You just want that feelingâof being scared ag...</td>\n",
       "      <td>ew</td>\n",
       "      <td>\"You just want that feelingâof being scared ag...</td>\n",
       "      <td>102.61</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62042</th>\n",
       "      <td>Superman. I tried out for the part in NUM for ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>Superman. I tried out for the part in NUM for ...</td>\n",
       "      <td>102.81</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62044</th>\n",
       "      <td>The role has gotten more difficult over the la...</td>\n",
       "      <td>ew</td>\n",
       "      <td>The role has gotten more difficult over the la...</td>\n",
       "      <td>103.83</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62096</th>\n",
       "      <td>When you dress me like this! And when you pose...</td>\n",
       "      <td>ew</td>\n",
       "      <td>When you dress me like this! And when you pose...</td>\n",
       "      <td>104.94</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62160</th>\n",
       "      <td>Mike Myers wants a fourth Austin Powers movie,...</td>\n",
       "      <td>ew</td>\n",
       "      <td>Mike Myers wants a fourth Austin Powers movie,...</td>\n",
       "      <td>101.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62283</th>\n",
       "      <td>Thank you, Jack. I don't know why my husband l...</td>\n",
       "      <td>ew</td>\n",
       "      <td>Thank you, Jack. I don't know why my husband l...</td>\n",
       "      <td>104.94</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62292</th>\n",
       "      <td>Jack does not want to take anybody out on Frid...</td>\n",
       "      <td>ew</td>\n",
       "      <td>Jack does not want to take anybody out on Frid...</td>\n",
       "      <td>106.67</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62504</th>\n",
       "      <td>The Founder p. NUM â¢ Miss Sloane p. NUM â¢ Assa...</td>\n",
       "      <td>ew</td>\n",
       "      <td>The Founder p. NUM â¢ Miss Sloane p. NUM â¢ Assa...</td>\n",
       "      <td>100.14</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63142</th>\n",
       "      <td>I got an NUM combined on my SATs and could bar...</td>\n",
       "      <td>ew</td>\n",
       "      <td>I got an NUM combined on my SATs and could bar...</td>\n",
       "      <td>100.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63271</th>\n",
       "      <td>\"By the NUMth or NUMth take, I was no longer p...</td>\n",
       "      <td>ew</td>\n",
       "      <td>\"By the NUMth or NUMth take, I was no longer p...</td>\n",
       "      <td>105.66</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64227</th>\n",
       "      <td>I texted my wife too many smiley faces with he...</td>\n",
       "      <td>ew</td>\n",
       "      <td>I texted my wife too many smiley faces with he...</td>\n",
       "      <td>106.37</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64371</th>\n",
       "      <td>â¢ The MC, who scored his first Hot NUM No. 1 a...</td>\n",
       "      <td>ew</td>\n",
       "      <td>â¢ The MC, who scored his first Hot NUM No. 1 a...</td>\n",
       "      <td>103.83</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64523</th>\n",
       "      <td>When they asked me to play the lead instead of...</td>\n",
       "      <td>ew</td>\n",
       "      <td>When they asked me to play the lead instead of...</td>\n",
       "      <td>102.61</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64536</th>\n",
       "      <td>That cover of the Smiths' song was my idea. I ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>That cover of the Smiths' song was my idea. I ...</td>\n",
       "      <td>104.13</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64594</th>\n",
       "      <td>And they paid us, too! [Laughs] It's just cool...</td>\n",
       "      <td>ew</td>\n",
       "      <td>And they paid us, too! [Laughs] It's just cool...</td>\n",
       "      <td>104.64</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64601</th>\n",
       "      <td>I'm trying to get my horse to go this way. He'...</td>\n",
       "      <td>ew</td>\n",
       "      <td>I'm trying to get my horse to go this way. He'...</td>\n",
       "      <td>105.96</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64742</th>\n",
       "      <td>The episode is titled \"Somebody That I Used to...</td>\n",
       "      <td>ew</td>\n",
       "      <td>The episode is titled \"Somebody That I Used to...</td>\n",
       "      <td>105.35</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64780</th>\n",
       "      <td>\"I do,\" I said, just as softly. \"And you know ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>\"I do,\" I said, just as softly. \"And you know ...</td>\n",
       "      <td>105.66</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64784</th>\n",
       "      <td>\"I would have shot him myself, on sight.\" I bl...</td>\n",
       "      <td>ew</td>\n",
       "      <td>\"I would have shot him myself, on sight.\" I bl...</td>\n",
       "      <td>105.86</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows Ã 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content     source  \\\n",
       "427    The exhibition proceeds broadly chronologicall...  economist   \n",
       "7844   Westron wynde, when wilt thou blow, the small ...  economist   \n",
       "8479   Vladimir frowns and thinks. And then he clicks...  economist   \n",
       "11432  The raw components of the sneakers being produ...      wired   \n",
       "11447  #whatjayzsaidtosolange; coming within inches o...      wired   \n",
       "11655  THE PROOF Hawaii and Stephen F. Austin, two te...      wired   \n",
       "12279  Which reminds me. I've got a great idea for a ...      wired   \n",
       "12679  \"Your first place is home. Your second is work...      wired   \n",
       "12953  Do not read this in a house. Do not read this ...      wired   \n",
       "13598  But he felt that he had no choice. Two months ...      wired   \n",
       "13642  By June NUM it was a circle. No one takes full...      wired   \n",
       "13698  If those million-eyed, fast-moving, deep-seate...      wired   \n",
       "13699  But if the world's governments continue to ins...      wired   \n",
       "14015  I ask Ferren how he'll keep a 4-year-old from ...      wired   \n",
       "14054  AND THE WINNER IS: Mozart. Sure, the dubstep k...      wired   \n",
       "14317  The man, the myth, the legend: Bill Murray; Ju...      wired   \n",
       "14382  We're bothered by AIs that try to sound human....      wired   \n",
       "14529  Advil; Aleve; Diet Coke; Papalote salsa; Comfy...      wired   \n",
       "14780  Now here you are, on the cusp of carrying that...      wired   \n",
       "15183  Land quarries and riverbanks were once the mai...      wired   \n",
       "15265  Who gets the bigger room? Who gets to name the...      wired   \n",
       "15302  \"But I couldn't sell it at a gallery,\" I say. ...      wired   \n",
       "15511  Buying a grill used to be simple: You went to ...      wired   \n",
       "16081  Anaya stumbled back from the truck's cab, livi...      wired   \n",
       "16203  We see the middle-aged man crouching in pain, ...      wired   \n",
       "16571  âThis was my gift to you,â Kane shot back test...      wired   \n",
       "16590  Nestor was up more than $NUM. The Game King ri...      wired   \n",
       "16650  And so this issue of WIRED brings you NUM page...      wired   \n",
       "16715  FIRST COMPUTER: I got this old black Dell lapt...      wired   \n",
       "16778  [5] \"The water jug helps me track how much wat...      wired   \n",
       "...                                                  ...        ...   \n",
       "60917  I did, because I'm a shoplifter [laughs]. Ther...         ew   \n",
       "61069  AMONG THE OTHER GRAND ENDEAVORS SURE TO HAVE p...         ew   \n",
       "61139  \"Sometimes\" by Britney Spears. [Laughs] It hap...         ew   \n",
       "61217  I saw over NUM young men. Those were videotape...         ew   \n",
       "61238  While we were shooting School Ties, we all wen...         ew   \n",
       "61241  We filmed the tribunal scene all day long. Par...         ew   \n",
       "61354  Cloris came in like a whirlwind, all over the ...         ew   \n",
       "61359  I was concerned that if I did a nude scene it ...         ew   \n",
       "61367  Ben Johnson, man. He was so great. I got to dr...         ew   \n",
       "61373  I was mad that Peter only let me do that scene...         ew   \n",
       "61381  I didn't expect it at all. I got a dress and i...         ew   \n",
       "61851  \"You just want that feelingâof being scared ag...         ew   \n",
       "62042  Superman. I tried out for the part in NUM for ...         ew   \n",
       "62044  The role has gotten more difficult over the la...         ew   \n",
       "62096  When you dress me like this! And when you pose...         ew   \n",
       "62160  Mike Myers wants a fourth Austin Powers movie,...         ew   \n",
       "62283  Thank you, Jack. I don't know why my husband l...         ew   \n",
       "62292  Jack does not want to take anybody out on Frid...         ew   \n",
       "62504  The Founder p. NUM â¢ Miss Sloane p. NUM â¢ Assa...         ew   \n",
       "63142  I got an NUM combined on my SATs and could bar...         ew   \n",
       "63271  \"By the NUMth or NUMth take, I was no longer p...         ew   \n",
       "64227  I texted my wife too many smiley faces with he...         ew   \n",
       "64371  â¢ The MC, who scored his first Hot NUM No. 1 a...         ew   \n",
       "64523  When they asked me to play the lead instead of...         ew   \n",
       "64536  That cover of the Smiths' song was my idea. I ...         ew   \n",
       "64594  And they paid us, too! [Laughs] It's just cool...         ew   \n",
       "64601  I'm trying to get my horse to go this way. He'...         ew   \n",
       "64742  The episode is titled \"Somebody That I Used to...         ew   \n",
       "64780  \"I do,\" I said, just as softly. \"And you know ...         ew   \n",
       "64784  \"I would have shot him myself, on sight.\" I bl...         ew   \n",
       "\n",
       "                                    Content_Preprocessed  readability_f  \\\n",
       "427    The exhibition proceeds broadly chronologicall...         -46.27   \n",
       "7844   Westron wynde, when wilt thou blow, the small ...         100.58   \n",
       "8479   Vladimir frowns and thinks. And then he clicks...         116.35   \n",
       "11432  The raw components of the sneakers being produ...          -7.03   \n",
       "11447  #whatjayzsaidtosolange; coming within inches o...          -4.32   \n",
       "11655  THE PROOF Hawaii and Stephen F. Austin, two te...         102.61   \n",
       "12279  Which reminds me. I've got a great idea for a ...         111.07   \n",
       "12679  \"Your first place is home. Your second is work...         106.37   \n",
       "12953  Do not read this in a house. Do not read this ...         115.13   \n",
       "13598  But he felt that he had no choice. Two months ...         101.09   \n",
       "13642  By June NUM it was a circle. No one takes full...         103.32   \n",
       "13698  If those million-eyed, fast-moving, deep-seate...          -2.97   \n",
       "13699  But if the world's governments continue to ins...         -35.11   \n",
       "14015  I ask Ferren how he'll keep a 4-year-old from ...         100.78   \n",
       "14054  AND THE WINNER IS: Mozart. Sure, the dubstep k...         101.09   \n",
       "14317  The man, the myth, the legend: Bill Murray; Ju...         -11.42   \n",
       "14382  We're bothered by AIs that try to sound human....         102.61   \n",
       "14529  Advil; Aleve; Diet Coke; Papalote salsa; Comfy...         -68.60   \n",
       "14780  Now here you are, on the cusp of carrying that...         -19.54   \n",
       "15183  Land quarries and riverbanks were once the mai...         102.10   \n",
       "15265  Who gets the bigger room? Who gets to name the...         103.42   \n",
       "15302  \"But I couldn't sell it at a gallery,\" I say. ...         100.07   \n",
       "15511  Buying a grill used to be simple: You went to ...         101.60   \n",
       "16081  Anaya stumbled back from the truck's cab, livi...         106.37   \n",
       "16203  We see the middle-aged man crouching in pain, ...         101.60   \n",
       "16571  âThis was my gift to you,â Kane shot back test...         100.58   \n",
       "16590  Nestor was up more than $NUM. The Game King ri...         103.12   \n",
       "16650  And so this issue of WIRED brings you NUM page...         101.29   \n",
       "16715  FIRST COMPUTER: I got this old black Dell lapt...         100.07   \n",
       "16778  [5] \"The water jug helps me track how much wat...         101.60   \n",
       "...                                                  ...            ...   \n",
       "60917  I did, because I'm a shoplifter [laughs]. Ther...         102.61   \n",
       "61069  AMONG THE OTHER GRAND ENDEAVORS SURE TO HAVE p...          -1.95   \n",
       "61139  \"Sometimes\" by Britney Spears. [Laughs] It hap...         102.61   \n",
       "61217  I saw over NUM young men. Those were videotape...         103.42   \n",
       "61238  While we were shooting School Ties, we all wen...         105.35   \n",
       "61241  We filmed the tribunal scene all day long. Par...         100.17   \n",
       "61354  Cloris came in like a whirlwind, all over the ...         100.78   \n",
       "61359  I was concerned that if I did a nude scene it ...         108.23   \n",
       "61367  Ben Johnson, man. He was so great. I got to dr...         106.16   \n",
       "61373  I was mad that Peter only let me do that scene...         103.83   \n",
       "61381  I didn't expect it at all. I got a dress and i...         103.63   \n",
       "61851  \"You just want that feelingâof being scared ag...         102.61   \n",
       "62042  Superman. I tried out for the part in NUM for ...         102.81   \n",
       "62044  The role has gotten more difficult over the la...         103.83   \n",
       "62096  When you dress me like this! And when you pose...         104.94   \n",
       "62160  Mike Myers wants a fourth Austin Powers movie,...         101.09   \n",
       "62283  Thank you, Jack. I don't know why my husband l...         104.94   \n",
       "62292  Jack does not want to take anybody out on Frid...         106.67   \n",
       "62504  The Founder p. NUM â¢ Miss Sloane p. NUM â¢ Assa...         100.14   \n",
       "63142  I got an NUM combined on my SATs and could bar...         100.07   \n",
       "63271  \"By the NUMth or NUMth take, I was no longer p...         105.66   \n",
       "64227  I texted my wife too many smiley faces with he...         106.37   \n",
       "64371  â¢ The MC, who scored his first Hot NUM No. 1 a...         103.83   \n",
       "64523  When they asked me to play the lead instead of...         102.61   \n",
       "64536  That cover of the Smiths' song was my idea. I ...         104.13   \n",
       "64594  And they paid us, too! [Laughs] It's just cool...         104.64   \n",
       "64601  I'm trying to get my horse to go this way. He'...         105.96   \n",
       "64742  The episode is titled \"Somebody That I Used to...         105.35   \n",
       "64780  \"I do,\" I said, just as softly. \"And you know ...         105.66   \n",
       "64784  \"I would have shot him myself, on sight.\" I bl...         105.86   \n",
       "\n",
       "       readability_SMOG  \n",
       "427                 0.0  \n",
       "7844                0.0  \n",
       "8479                3.1  \n",
       "11432               0.0  \n",
       "11447               0.0  \n",
       "11655               0.0  \n",
       "12279               0.0  \n",
       "12679               3.1  \n",
       "12953               3.1  \n",
       "13598               0.0  \n",
       "13642               3.1  \n",
       "13698               0.0  \n",
       "13699               0.0  \n",
       "14015               6.0  \n",
       "14054               0.0  \n",
       "14317               0.0  \n",
       "14382               0.0  \n",
       "14529               0.0  \n",
       "14780               0.0  \n",
       "15183               0.0  \n",
       "15265               5.7  \n",
       "15302               0.0  \n",
       "15511               6.0  \n",
       "16081               6.0  \n",
       "16203               3.1  \n",
       "16571               0.0  \n",
       "16590               0.0  \n",
       "16650               6.4  \n",
       "16715               0.0  \n",
       "16778               0.0  \n",
       "...                 ...  \n",
       "60917               0.0  \n",
       "61069               0.0  \n",
       "61139               3.1  \n",
       "61217               5.7  \n",
       "61238               3.1  \n",
       "61241               5.7  \n",
       "61354               5.4  \n",
       "61359               3.1  \n",
       "61367               3.1  \n",
       "61373               3.1  \n",
       "61381               3.1  \n",
       "61851               5.7  \n",
       "62042               6.0  \n",
       "62044               6.0  \n",
       "62096               3.1  \n",
       "62160               0.0  \n",
       "62283               3.1  \n",
       "62292               3.1  \n",
       "62504               6.1  \n",
       "63142               0.0  \n",
       "63271               6.4  \n",
       "64227               3.1  \n",
       "64371               3.1  \n",
       "64523               3.1  \n",
       "64536               3.1  \n",
       "64594               6.4  \n",
       "64601               3.1  \n",
       "64742               3.1  \n",
       "64780               3.1  \n",
       "64784               3.1  \n",
       "\n",
       "[262 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need to explore this futher. Brought it down to 230 from over 2.5k\n",
    "final_df_filtered[(final_df_filtered['readability_f']<0)|(final_df_filtered['readability_f']>100) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMeanSentLen(para):\n",
    "    #Count number of sentences\n",
    "    sent_tok = nltk.sent_tokenize(para)\n",
    "    ns = len(sent_tok)\n",
    "    \n",
    "    #Count number of words\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    \n",
    "    ##Average Sentence length are words divided by sentences\n",
    "    avg=nw/ns\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSDSentLen(para):\n",
    "    #Count number of sentences\n",
    "    sent_tok = nltk.sent_tokenize(para)\n",
    "    ns = len(sent_tok)\n",
    "    \n",
    "    #Count number of words\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    \n",
    "    ##Average Sentence length are words divided by sentences\n",
    "    avg=nw/ns\n",
    "    #print(avg)\n",
    "    sum1=0\n",
    "    #Standard Deviation \n",
    "    for sent in sent_tok:\n",
    "        sum1=sum1+(len(sent.split())-avg)**2\n",
    "        #print(len(sent.split()))\n",
    "        #print(sum1)\n",
    "    sd=(sum1/ns)**0.5\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normStopWordFrequency(para):\n",
    "    stopwords1=set(stopwords.words('english'))\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    word_tok_stop=[i for i in word_tok if i.lower() in stopwords1]\n",
    "    n_stop=len(word_tok_stop)\n",
    "    return(n_stop/nw)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normFunctWordFrequency(functional,para):\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    word_tok_funct=[i for i in word_tok if i.lower() in functional]\n",
    "    n_funct=len(word_tok_funct)\n",
    "    return(n_funct/nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normPunctFrequency(para):\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "                               \n",
    "    no_punct = count(para, string.punctuation)\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    return(no_punct/nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.421875"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normStopWordFrequency(final_df_filtered.loc[0,'Content_Preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.86029791643813"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSDSentLen(final_df_filtered.loc[0,'Content_Preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#final_df1['avg_sent_len'],final_df1['sd_sent_len'] = final_df1['Content_Preprocessed'].apply(lambda x:getMeanSDSentLen(x))\n",
    "\n",
    "final_df_filtered['avg_sent_len'] = final_df_filtered.apply(lambda row: getMeanSentLen(row['Content_Preprocessed']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "final_df_filtered['sd_sent_len'] = final_df_filtered.apply(lambda row: getSDSentLen(row['Content_Preprocessed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "final_df_filtered['norm_stop_freq'] = final_df_filtered.apply(lambda row: normStopWordFrequency(row['Content_Preprocessed']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "final_df_filtered['norm_punct_freq'] = final_df_filtered.apply(lambda row: normPunctFrequency(row['Content_Preprocessed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['against', 'though', 'while', 'also', 'any', 'thus', 'is', 'should', 'must', 'nevertheless', 'four', 'a', 'some', 'from', 'our', 'until', 'further', 'nonetheless', 'do', 'why', 'via', 'nine', 'ok', 'his', 'behind', 'by', 'this', 'past', 'into', 'and', 'three', 'at', 'underneath', 'himself', 'opposite', 'can', 'however', 'across', 'whatever', 'whom', 'around', 'most', 'up', 'moreover', 'that', 'over', 'conversely', 'either', 'on', 'whether', 'because', 'as', 'than', 'ought', 'much', 'yourself', 'except', 'eight', 'otherwise', 'all', 'their', 'besides', 'like', 'third', 'despite', 'had', 'those', 'many', 'considering', 'him', 'excluding', 'within', 'these', 'upon', 'little', 'course', 'near', 'since', 'toward', 'such', 'onto', 'previous', 'enough', 'concerning', 'herself', 'he', 'theirs', 'I', 'six', 'have', 'versus', 'if', 'five', 'yes', 'neither', 'ours', 'about', 'before', 'not', 'below', 'between', 'meanwhile', 'are', 'inside', 'unlike', 'which', 'then', 'with', 'being', 'wherever', 'last', 'oh', 'beyond', 'ten', 'lot', 'has', 'first', 'during', 'how', 'whenever', 'other', 'every', 'mine', 'her', 'who', 'aboard', 'outside', 'lest', 'she', 'whereas', 'instead', 'accordingly', 'second', 'regarding', 'okay', 'myself', 'hence', 'nowhere', 'next', 'beneath', 'without', 'been', 'having', 'among', 'for', 'although', 'above', 'could', 'what', 'the', 'in', 'consequently', 'did', 'after', 'only', 'off', 'round', 'shall', 'each', 'both', 'of', 'till', 'doing', 'two', 'no', 'when', 'plus', 'might', 'along', 'thirty', 'following', 'was', 'were', 'we', 'somewhere', 'whomever', 'me', 'my', 'anti', 'am', 'itself', 'you', 'it', 'half', 'one', 'under', 'be', 'few', 'likewise', 'themselves', 'hers', 'done', 'once', 'excepting', 'per', 'unless', 'still', 'whoever', 'yeah', 'an', 'may', 'down', 'does', 'another', 'whose', 'but', 'hardly', 'sure', 'would', 'minus', 'even', 'will', 'towards', 'them', 'quite', 'therefore', 'yours', 'seven', 'your', 'ourselves', 'amid', 'us', 'they', 'through', 'rather', 'to', 'furthermore', 'its', 'beside', 'whichever']\n"
     ]
    }
   ],
   "source": [
    "functional_file = open(\"functional.txt\", \"r\")\n",
    "words= [word.strip() for line in functional_file.readlines() for word in line.split(',') if word.strip()]\n",
    "functional=list(set(words))\n",
    "print((functional))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "final_df_filtered['norm_funct_freq'] = final_df_filtered.apply(lambda row: normFunctWordFrequency(functional,row['Content_Preprocessed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "      <th>Content_Preprocessed</th>\n",
       "      <th>readability_f</th>\n",
       "      <th>readability_SMOG</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>sd_sent_len</th>\n",
       "      <th>norm_stop_freq</th>\n",
       "      <th>norm_punct_freq</th>\n",
       "      <th>norm_funct_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOWN the Euphrates river, halfway between Deir...</td>\n",
       "      <td>economist</td>\n",
       "      <td>DOWN the Euphrates river, halfway between Deir...</td>\n",
       "      <td>50.46</td>\n",
       "      <td>11.2</td>\n",
       "      <td>21.333333</td>\n",
       "      <td>11.860298</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Never have America and its allies had such a h...</td>\n",
       "      <td>economist</td>\n",
       "      <td>Never have America and its allies had such a h...</td>\n",
       "      <td>62.68</td>\n",
       "      <td>10.7</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.391165</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But like their Parthian forebears, Iran and it...</td>\n",
       "      <td>economist</td>\n",
       "      <td>But like their Parthian forebears, Iran and it...</td>\n",
       "      <td>91.31</td>\n",
       "      <td>5.7</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>5.858327</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.436620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iran's gains are even more striking elsewhere....</td>\n",
       "      <td>economist</td>\n",
       "      <td>Iran's gains are even more striking elsewhere....</td>\n",
       "      <td>61.87</td>\n",
       "      <td>11.6</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>7.628892</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Farther south, America's hopes of stemming Ira...</td>\n",
       "      <td>economist</td>\n",
       "      <td>Farther south, America's hopes of stemming Ira...</td>\n",
       "      <td>52.49</td>\n",
       "      <td>13.6</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7.527727</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content     source  \\\n",
       "0  DOWN the Euphrates river, halfway between Deir...  economist   \n",
       "1  Never have America and its allies had such a h...  economist   \n",
       "2  But like their Parthian forebears, Iran and it...  economist   \n",
       "3  Iran's gains are even more striking elsewhere....  economist   \n",
       "4  Farther south, America's hopes of stemming Ira...  economist   \n",
       "\n",
       "                                Content_Preprocessed  readability_f  \\\n",
       "0  DOWN the Euphrates river, halfway between Deir...          50.46   \n",
       "1  Never have America and its allies had such a h...          62.68   \n",
       "2  But like their Parthian forebears, Iran and it...          91.31   \n",
       "3  Iran's gains are even more striking elsewhere....          61.87   \n",
       "4  Farther south, America's hopes of stemming Ira...          52.49   \n",
       "\n",
       "   readability_SMOG  avg_sent_len  sd_sent_len  norm_stop_freq  \\\n",
       "0              11.2     21.333333    11.860298        0.421875   \n",
       "1              10.7     17.000000     3.391165        0.500000   \n",
       "2               5.7     14.200000     5.858327        0.422535   \n",
       "3              11.6     18.000000     7.628892        0.422222   \n",
       "4              13.6     20.000000     7.527727        0.366667   \n",
       "\n",
       "   norm_punct_freq  norm_funct_freq  \n",
       "0         0.234375         0.390625  \n",
       "1         0.132353         0.470588  \n",
       "2         0.197183         0.436620  \n",
       "3         0.255556         0.411111  \n",
       "4         0.316667         0.350000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_features= final_df_filtered[['source','readability_f','readability_SMOG','avg_sent_len','sd_sent_len','norm_stop_freq','norm_punct_freq','norm_funct_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>readability_f</th>\n",
       "      <th>readability_SMOG</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>sd_sent_len</th>\n",
       "      <th>norm_stop_freq</th>\n",
       "      <th>norm_punct_freq</th>\n",
       "      <th>norm_funct_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>economist</td>\n",
       "      <td>50.46</td>\n",
       "      <td>11.2</td>\n",
       "      <td>21.333333</td>\n",
       "      <td>11.860298</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economist</td>\n",
       "      <td>62.68</td>\n",
       "      <td>10.7</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.391165</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>economist</td>\n",
       "      <td>91.31</td>\n",
       "      <td>5.7</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>5.858327</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.436620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>economist</td>\n",
       "      <td>61.87</td>\n",
       "      <td>11.6</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>7.628892</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economist</td>\n",
       "      <td>52.49</td>\n",
       "      <td>13.6</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>7.527727</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source  readability_f  readability_SMOG  avg_sent_len  sd_sent_len  \\\n",
       "0  economist          50.46              11.2     21.333333    11.860298   \n",
       "1  economist          62.68              10.7     17.000000     3.391165   \n",
       "2  economist          91.31               5.7     14.200000     5.858327   \n",
       "3  economist          61.87              11.6     18.000000     7.628892   \n",
       "4  economist          52.49              13.6     20.000000     7.527727   \n",
       "\n",
       "   norm_stop_freq  norm_punct_freq  norm_funct_freq  \n",
       "0        0.421875         0.234375         0.390625  \n",
       "1        0.500000         0.132353         0.470588  \n",
       "2        0.422535         0.197183         0.436620  \n",
       "3        0.422222         0.255556         0.411111  \n",
       "4        0.366667         0.316667         0.350000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X -> features, y -> label\n",
    "X=df_features[['readability_f','readability_SMOG','avg_sent_len','sd_sent_len','norm_stop_freq','norm_punct_freq','norm_funct_freq']]\n",
    "y=df_features.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dividing X, y into train and test data \n",
    "#We use stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "svm_predictions = svm_model_linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = svm_model_linear.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, svm_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_model_radial = SVC(kernel = 'rbf', C = 1).fit(X_train, y_train)\n",
    "svm_predictions = svm_model_radial.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = svm_model_radial.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOME OTHER KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
