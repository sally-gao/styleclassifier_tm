{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(path, replace_dict, min_length=10):\n",
    "    \"\"\"\n",
    "    Removes source-specific artifacts from documents.\n",
    "    \n",
    "    :param path: filepath containing JSON files for each document\n",
    "    :param replace_dict: dictionary containing regex matching strings and strings to replace them with\n",
    "    :param min_length: minimum document length\n",
    "    \n",
    "    :returns: list of strings, each containing document content\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = []\n",
    "    #files = os.listdir(path)\n",
    "    json_files = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n",
    "\n",
    "    for file in json_files:\n",
    "        #file = '\\\\' + file\n",
    "        content = json.load(open(path + file))['content']\n",
    "        \n",
    "        # replace regex strings\n",
    "        for key, value in replace_dict.items():\n",
    "            content = re.sub(key, value, content)\n",
    "        \n",
    "        # remove small documents\n",
    "        if len(content) >= min_length:\n",
    "            docs.append(content)\n",
    "        \n",
    "    return(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "economist_path = 'data_updated/economist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "economist_dict = {}\n",
    "\n",
    "# artifacts on accented letters\n",
    "economist_dict['AaAaAeA '] = 'i'\n",
    "economist_dict['AaAaAeAo'] = 'c'\n",
    "economist_dict['AaAaAeAc'] = 'a'\n",
    "economist_dict['AaAaAeA~'] = 'n'\n",
    "economist_dict['AaAaAeA@|AaAaAeA\\?|AaAaAeA{|AaAaAe'] = 'e'\n",
    "\n",
    "\n",
    "\n",
    "# numbers\n",
    "economist_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "\n",
    "# Go online artifacts\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "economist_dict['<p>Go online ([^<]*)</p>|<p>([^<]*)([^.?!\"]){1}</p>'] = ''\n",
    "\n",
    "# end of paragraph tags\n",
    "economist_dict['</p>'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "economist_docs = process_corpus(economist_path, economist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104\n"
     ]
    }
   ],
   "source": [
    "print(len(economist_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "economist_paragraphs = []\n",
    "for doc in economist_docs:\n",
    "    economist_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "economist_paragraphs = [doc.strip() for doc in economist_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11198"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(economist_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wired_path = 'data_updated/wired/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wired_dict = {}\n",
    "\n",
    "# numbers\n",
    "wired_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "# author/subject descriptions at end of article\n",
    "# paragraph symbols\n",
    "wired_dict['<p>([^<]*)([^.?!\"]){1}</p>|<p>([^<]*)([A-Z]+) \\(@(.*)|¶'] = ''\n",
    "\n",
    "#email addresses\n",
    "wired_dict['[\\w\\.-]+@[\\w\\.-]+']=''\n",
    "\n",
    "# end of paragraph tags\n",
    "wired_dict['</p>'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wired_docs = process_corpus(wired_path, wired_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296\n"
     ]
    }
   ],
   "source": [
    "print(len(wired_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wired_paragraphs = []\n",
    "for doc in wired_docs:\n",
    "    wired_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "wired_paragraphs = [doc.strip() for doc in wired_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17142"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wired_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Yorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyorker_path = 'data_updated/newyorker/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyorker_dict = {}\n",
    "\n",
    "# artifacts on accented letters\n",
    "newyorker_dict['AaAaAeA '] = 'i'\n",
    "newyorker_dict['AaAaAeAo'] = 'c'\n",
    "newyorker_dict['AaAaAeAc'] = 'a'\n",
    "newyorker_dict['AaAaAeA~'] = 'n'\n",
    "newyorker_dict['AaAaAeA@|AaAaAeA\\?|AaAaAeA{|AaAaAe'] = 'e'\n",
    "\n",
    "# numbers\n",
    "newyorker_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "# bylines\n",
    "newyorker_dict['<p>([^<]*)([^.?!\"]){1}</p>|<p>Byline([^<]*)</p>'] = ''\n",
    "\n",
    "# end of paragraph tags\n",
    "newyorker_dict['</p>'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyorker_docs = process_corpus(newyorker_path, newyorker_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807\n"
     ]
    }
   ],
   "source": [
    "print(len(newyorker_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyorker_paragraphs = []\n",
    "for doc in newyorker_docs:\n",
    "    newyorker_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "newyorker_paragraphs = [doc.strip() for doc in newyorker_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19030"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newyorker_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_path = 'data_updated/ew/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_dict = {}\n",
    "\n",
    "# artifacts on accented letters\n",
    "ew_dict['AaAaAeA '] = 'i'\n",
    "ew_dict['AaAaAeAo'] = 'c'\n",
    "ew_dict['AaAaAeAc'] = 'a'\n",
    "ew_dict['AaAaAeA~'] = 'n'\n",
    "ew_dict['AaAaAeA@|AaAaAeA\\?|AaAaAeA{|AaAaAe'] = 'e'\n",
    "\n",
    "# numbers\n",
    "ew_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "#email addresses\n",
    "ew_dict['[\\w\\.-]+@[\\w\\.-]+']=''\n",
    "\n",
    "# Go online artifacts\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "#ew_dict['<p>Go online ([^<]*)</p>|<p>([^<]*)([^.?!+\"]){1}</p>'] = ''\n",
    "\n",
    "#bullet points\n",
    "ew_dict['\\xc2\\xb7']=''\n",
    "\n",
    "# end of paragraph tags\n",
    "ew_dict['</p>'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_docs = process_corpus(ew_path, ew_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2190\n"
     ]
    }
   ],
   "source": [
    "print(len(ew_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_paragraphs = []\n",
    "for doc in ew_docs:\n",
    "    ew_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "ew_paragraphs = [doc.strip() for doc in ew_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17536\n"
     ]
    }
   ],
   "source": [
    "print(len(ew_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = economist_paragraphs + wired_paragraphs + newyorker_paragraphs+ew_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = list(itertools.repeat('economist', len(economist_paragraphs)))\n",
    "sources += list(itertools.repeat('wired', len(wired_paragraphs)))\n",
    "sources += list(itertools.repeat('newyorker', len(newyorker_paragraphs)))\n",
    "sources +=list(itertools.repeat('ew',len(ew_paragraphs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'content':paragraphs, 'source':sources}\n",
    "final_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64906"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOWN the Euphrates river, halfway between Deir...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Never have America and its allies had such a h...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But like their Parthian forebears, Iran and it...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iran's gains are even more striking elsewhere....</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Farther south, America's hopes of stemming Ira...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content     source\n",
       "0  DOWN the Euphrates river, halfway between Deir...  economist\n",
       "1  Never have America and its allies had such a h...  economist\n",
       "2  But like their Parthian forebears, Iran and it...  economist\n",
       "3  Iran's gains are even more striking elsewhere....  economist\n",
       "4  Farther south, America's hopes of stemming Ira...  economist"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final_df.to_csv('pre_pre_processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHTMLTags(x):\n",
    "    content = re.sub(\"(?i)<\\/?\\w+((\\s+\\w+(\\s*=\\s*(?:\\\".*?\\\"|'.*?'|[^'\\\">\\s]+))?)+\\s*|\\s*)\\/?>\", '', x)\n",
    "    #content1 = re.sub(\"\\\\b\\\\x94\\\\b\", ' ', content)\n",
    "    \n",
    "    return(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Content_Preprocessed']=final_df['content'].apply(lambda x:removeHTMLTags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    DOWN the Euphrates river, halfway between Deir...\n",
       "1    Never have America and its allies had such a h...\n",
       "2    But like their Parthian forebears, Iran and it...\n",
       "3    Iran's gains are even more striking elsewhere....\n",
       "4    Farther south, America's hopes of stemming Ira...\n",
       "Name: Content_Preprocessed, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['Content_Preprocessed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1 = final_df[final_df['Content_Preprocessed'].apply(lambda x:len(x.split()))>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55136"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence-Level Preprocessing Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1['readability_f'] = final_df1['Content_Preprocessed'].apply(textstat.flesch_reading_ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1['readability_SMOG']=final_df1['Content_Preprocessed'].apply(textstat.smog_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49339.000000\n",
       "mean        62.872928\n",
       "std         14.706272\n",
       "min        -46.270000\n",
       "25%         53.440000\n",
       "50%         63.190000\n",
       "75%         72.760000\n",
       "max        116.350000\n",
       "Name: readability_f, dtype: float64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df1['readability_f'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "      <th>readability_f</th>\n",
       "      <th>Content_Preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>Vladimir frowns and thinks. And then he clicks...</td>\n",
       "      <td>economist</td>\n",
       "      <td>116.35</td>\n",
       "      <td>Vladimir frowns and thinks. And then he clicks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6436</th>\n",
       "      <td>The exhibition proceeds broadly chronologicall...</td>\n",
       "      <td>economist</td>\n",
       "      <td>-46.27</td>\n",
       "      <td>The exhibition proceeds broadly chronologicall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10091</th>\n",
       "      <td>Westron wynde, when wilt thou blow, the small ...</td>\n",
       "      <td>economist</td>\n",
       "      <td>100.58</td>\n",
       "      <td>Westron wynde, when wilt thou blow, the small ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11424</th>\n",
       "      <td>[1] A tiny suction hose pulls out and holds th...</td>\n",
       "      <td>wired</td>\n",
       "      <td>101.09</td>\n",
       "      <td>[1] A tiny suction hose pulls out and holds th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11508</th>\n",
       "      <td>They were in luck--the attendant gassed up the...</td>\n",
       "      <td>wired</td>\n",
       "      <td>101.60</td>\n",
       "      <td>They were in luck--the attendant gassed up the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11668</th>\n",
       "      <td>[5] \"The water jug helps me track how much wat...</td>\n",
       "      <td>wired</td>\n",
       "      <td>101.60</td>\n",
       "      <td>[5] \"The water jug helps me track how much wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>Land quarries and riverbanks were once the mai...</td>\n",
       "      <td>wired</td>\n",
       "      <td>102.10</td>\n",
       "      <td>Land quarries and riverbanks were once the mai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11878</th>\n",
       "      <td>DREAD PIRATE ROBERTS 3/NUM/NUM 8:NUM Don't wan...</td>\n",
       "      <td>wired</td>\n",
       "      <td>100.88</td>\n",
       "      <td>DREAD PIRATE ROBERTS 3/NUM/NUM 8:NUM Don't wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>Lucas: We had NUM shots to get through. They'd...</td>\n",
       "      <td>wired</td>\n",
       "      <td>104.44</td>\n",
       "      <td>Lucas: We had NUM shots to get through. They'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12044</th>\n",
       "      <td>Dippé: We made so much noise that they put us ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>102.10</td>\n",
       "      <td>Dippé: We made so much noise that they put us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12072</th>\n",
       "      <td>Kennedy: ILM did a proof-of-concept shot for T...</td>\n",
       "      <td>wired</td>\n",
       "      <td>104.64</td>\n",
       "      <td>Kennedy: ILM did a proof-of-concept shot for T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12082</th>\n",
       "      <td>Jaeger: I've worked on seven of his films now....</td>\n",
       "      <td>wired</td>\n",
       "      <td>102.10</td>\n",
       "      <td>Jaeger: I've worked on seven of his films now....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12393</th>\n",
       "      <td>DREAD: well, I'm also learning who I am. I don...</td>\n",
       "      <td>wired</td>\n",
       "      <td>103.12</td>\n",
       "      <td>DREAD: well, I'm also learning who I am. I don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12653</th>\n",
       "      <td>What went wrong? I had used the best Belgian b...</td>\n",
       "      <td>wired</td>\n",
       "      <td>105.35</td>\n",
       "      <td>What went wrong? I had used the best Belgian b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12763</th>\n",
       "      <td>Now here you are, on the cusp of carrying that...</td>\n",
       "      <td>wired</td>\n",
       "      <td>-19.54</td>\n",
       "      <td>Now here you are, on the cusp of carrying that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12967</th>\n",
       "      <td>And so this issue of WIRED brings you NUM page...</td>\n",
       "      <td>wired</td>\n",
       "      <td>101.29</td>\n",
       "      <td>And so this issue of WIRED brings you NUM page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13982</th>\n",
       "      <td>It's all up to you. Only you can save it. Only...</td>\n",
       "      <td>wired</td>\n",
       "      <td>105.35</td>\n",
       "      <td>It's all up to you. Only you can save it. Only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14338</th>\n",
       "      <td>The secret burrito compartment in my car's das...</td>\n",
       "      <td>wired</td>\n",
       "      <td>-35.44</td>\n",
       "      <td>The secret burrito compartment in my car's das...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14803</th>\n",
       "      <td>The man, the myth, the legend: Bill Murray; Ju...</td>\n",
       "      <td>wired</td>\n",
       "      <td>-11.42</td>\n",
       "      <td>The man, the myth, the legend: Bill Murray; Ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14872</th>\n",
       "      <td>A seafood allergy; a Penicillium allergy (no b...</td>\n",
       "      <td>wired</td>\n",
       "      <td>-36.80</td>\n",
       "      <td>A seafood allergy; a Penicillium allergy (no b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14936</th>\n",
       "      <td>So over the course of one strange week in San ...</td>\n",
       "      <td>wired</td>\n",
       "      <td>101.60</td>\n",
       "      <td>So over the course of one strange week in San ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15399</th>\n",
       "      <td>Who gets the bigger room? Who gets to name the...</td>\n",
       "      <td>wired</td>\n",
       "      <td>103.42</td>\n",
       "      <td>Who gets the bigger room? Who gets to name the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15410</th>\n",
       "      <td>There are bad parts. Say you go to bed late, b...</td>\n",
       "      <td>wired</td>\n",
       "      <td>105.66</td>\n",
       "      <td>There are bad parts. Say you go to bed late, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15430</th>\n",
       "      <td>Use a Kura bed as your frame and solid pine Iv...</td>\n",
       "      <td>wired</td>\n",
       "      <td>100.58</td>\n",
       "      <td>Use a Kura bed as your frame and solid pine Iv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15669</th>\n",
       "      <td>But then I ask him how he feels about Lee’s de...</td>\n",
       "      <td>wired</td>\n",
       "      <td>113.81</td>\n",
       "      <td>But then I ask him how he feels about Lee’s de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16300</th>\n",
       "      <td>Start recording and put your phone down. If yo...</td>\n",
       "      <td>wired</td>\n",
       "      <td>100.78</td>\n",
       "      <td>Start recording and put your phone down. If yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16657</th>\n",
       "      <td>Pack everything you think you need, then get r...</td>\n",
       "      <td>wired</td>\n",
       "      <td>100.88</td>\n",
       "      <td>Pack everything you think you need, then get r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16840</th>\n",
       "      <td>Over the years, the increasingly complicated c...</td>\n",
       "      <td>wired</td>\n",
       "      <td>-6.70</td>\n",
       "      <td>Over the years, the increasingly complicated c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17090</th>\n",
       "      <td>Fridges will be useless once the grid gives ou...</td>\n",
       "      <td>wired</td>\n",
       "      <td>102.10</td>\n",
       "      <td>Fridges will be useless once the grid gives ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17177</th>\n",
       "      <td>NUM–APRIL NUM (AT LEAST) Targets_____ Westingh...</td>\n",
       "      <td>wired</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>NUM–APRIL NUM (AT LEAST) Targets_____ Westingh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54534</th>\n",
       "      <td>Moonlight teens win MTV award for Best Kiss. I...</td>\n",
       "      <td>ew</td>\n",
       "      <td>102.10</td>\n",
       "      <td>Moonlight teens win MTV award for Best Kiss. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54688</th>\n",
       "      <td>I got to the point where she pulls the boy's l...</td>\n",
       "      <td>ew</td>\n",
       "      <td>102.10</td>\n",
       "      <td>I got to the point where she pulls the boy's l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54730</th>\n",
       "      <td>Mike Myers wants a fourth Austin Powers movie,...</td>\n",
       "      <td>ew</td>\n",
       "      <td>101.09</td>\n",
       "      <td>Mike Myers wants a fourth Austin Powers movie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54872</th>\n",
       "      <td>It was the kind of venue where you wafted. Fle...</td>\n",
       "      <td>ew</td>\n",
       "      <td>107.69</td>\n",
       "      <td>It was the kind of venue where you wafted. Fle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54881</th>\n",
       "      <td>I didn't think of anything beyond the play at ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>102.31</td>\n",
       "      <td>I didn't think of anything beyond the play at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54894</th>\n",
       "      <td>That's the best way to be. I have never felt l...</td>\n",
       "      <td>ew</td>\n",
       "      <td>100.58</td>\n",
       "      <td>That's the best way to be. I have never felt l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54956</th>\n",
       "      <td>Superman. I tried out for the part in NUM for ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>102.81</td>\n",
       "      <td>Superman. I tried out for the part in NUM for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54958</th>\n",
       "      <td>The role has gotten more difficult over the la...</td>\n",
       "      <td>ew</td>\n",
       "      <td>103.83</td>\n",
       "      <td>The role has gotten more difficult over the la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55090</th>\n",
       "      <td>\"The songs tend to change quite a lot. I mean,...</td>\n",
       "      <td>ew</td>\n",
       "      <td>101.80</td>\n",
       "      <td>\"The songs tend to change quite a lot. I mean,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55529</th>\n",
       "      <td>Cloris came in like a whirlwind, all over the ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>100.78</td>\n",
       "      <td>Cloris came in like a whirlwind, all over the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55534</th>\n",
       "      <td>I was concerned that if I did a nude scene it ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>108.23</td>\n",
       "      <td>I was concerned that if I did a nude scene it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55542</th>\n",
       "      <td>Ben Johnson, man. He was so great. I got to dr...</td>\n",
       "      <td>ew</td>\n",
       "      <td>106.16</td>\n",
       "      <td>Ben Johnson, man. He was so great. I got to dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55548</th>\n",
       "      <td>I was mad that Peter only let me do that scene...</td>\n",
       "      <td>ew</td>\n",
       "      <td>103.83</td>\n",
       "      <td>I was mad that Peter only let me do that scene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55556</th>\n",
       "      <td>I didn't expect it at all. I got a dress and i...</td>\n",
       "      <td>ew</td>\n",
       "      <td>103.63</td>\n",
       "      <td>I didn't expect it at all. I got a dress and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55656</th>\n",
       "      <td>Still, \"that was a very big day for me,\" Thoma...</td>\n",
       "      <td>ew</td>\n",
       "      <td>103.32</td>\n",
       "      <td>Still, \"that was a very big day for me,\" Thoma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55718</th>\n",
       "      <td>I was on tour [in NUM] and we had a day off. W...</td>\n",
       "      <td>ew</td>\n",
       "      <td>101.60</td>\n",
       "      <td>I was on tour [in NUM] and we had a day off. W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55776</th>\n",
       "      <td>\"I START MAKING THE PHONE call. It rings. Made...</td>\n",
       "      <td>ew</td>\n",
       "      <td>100.38</td>\n",
       "      <td>\"I START MAKING THE PHONE call. It rings. Made...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55780</th>\n",
       "      <td>I was a bookworm. I ate books. I read so many,...</td>\n",
       "      <td>ew</td>\n",
       "      <td>105.96</td>\n",
       "      <td>I was a bookworm. I ate books. I read so many,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55818</th>\n",
       "      <td>In that way, Five Came Back is not just a war ...</td>\n",
       "      <td>ew</td>\n",
       "      <td>104.94</td>\n",
       "      <td>In that way, Five Came Back is not just a war ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55919</th>\n",
       "      <td>I am grateful—I never thought we would be here...</td>\n",
       "      <td>ew</td>\n",
       "      <td>102.91</td>\n",
       "      <td>I am grateful—I never thought we would be here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56236</th>\n",
       "      <td>Thank you, Jack. I don't know why my husband l...</td>\n",
       "      <td>ew</td>\n",
       "      <td>104.94</td>\n",
       "      <td>Thank you, Jack. I don't know why my husband l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56245</th>\n",
       "      <td>Jack does not want to take anybody out on Frid...</td>\n",
       "      <td>ew</td>\n",
       "      <td>106.67</td>\n",
       "      <td>Jack does not want to take anybody out on Frid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56511</th>\n",
       "      <td>And with those words (from the ghost of her de...</td>\n",
       "      <td>ew</td>\n",
       "      <td>101.09</td>\n",
       "      <td>And with those words (from the ghost of her de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56683</th>\n",
       "      <td>When you dress me like this! And when you pose...</td>\n",
       "      <td>ew</td>\n",
       "      <td>104.94</td>\n",
       "      <td>When you dress me like this! And when you pose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56758</th>\n",
       "      <td>She's played twice. She's won twice. Sandra kn...</td>\n",
       "      <td>ew</td>\n",
       "      <td>103.12</td>\n",
       "      <td>She's played twice. She's won twice. Sandra kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57074</th>\n",
       "      <td>La La Land scored NUM noms. It will need to wi...</td>\n",
       "      <td>ew</td>\n",
       "      <td>102.10</td>\n",
       "      <td>La La Land scored NUM noms. It will need to wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57447</th>\n",
       "      <td>It was also painful for the actors, who had to...</td>\n",
       "      <td>ew</td>\n",
       "      <td>103.63</td>\n",
       "      <td>It was also painful for the actors, who had to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57572</th>\n",
       "      <td>No one will have to fight these two for a spot...</td>\n",
       "      <td>ew</td>\n",
       "      <td>101.60</td>\n",
       "      <td>No one will have to fight these two for a spot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57903</th>\n",
       "      <td>You move to L.A. To make ends meet, you take j...</td>\n",
       "      <td>ew</td>\n",
       "      <td>100.54</td>\n",
       "      <td>You move to L.A. To make ends meet, you take j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57904</th>\n",
       "      <td>Then one day, a miracle. You get your break. A...</td>\n",
       "      <td>ew</td>\n",
       "      <td>108.40</td>\n",
       "      <td>Then one day, a miracle. You get your break. A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content     source  \\\n",
       "2102   Vladimir frowns and thinks. And then he clicks...  economist   \n",
       "6436   The exhibition proceeds broadly chronologicall...  economist   \n",
       "10091  Westron wynde, when wilt thou blow, the small ...  economist   \n",
       "11424  [1] A tiny suction hose pulls out and holds th...      wired   \n",
       "11508  They were in luck--the attendant gassed up the...      wired   \n",
       "11668  [5] \"The water jug helps me track how much wat...      wired   \n",
       "11819  Land quarries and riverbanks were once the mai...      wired   \n",
       "11878  DREAD PIRATE ROBERTS 3/NUM/NUM 8:NUM Don't wan...      wired   \n",
       "11998  Lucas: We had NUM shots to get through. They'd...      wired   \n",
       "12044  Dippé: We made so much noise that they put us ...      wired   \n",
       "12072  Kennedy: ILM did a proof-of-concept shot for T...      wired   \n",
       "12082  Jaeger: I've worked on seven of his films now....      wired   \n",
       "12393  DREAD: well, I'm also learning who I am. I don...      wired   \n",
       "12653  What went wrong? I had used the best Belgian b...      wired   \n",
       "12763  Now here you are, on the cusp of carrying that...      wired   \n",
       "12967  And so this issue of WIRED brings you NUM page...      wired   \n",
       "13982  It's all up to you. Only you can save it. Only...      wired   \n",
       "14338  The secret burrito compartment in my car's das...      wired   \n",
       "14803  The man, the myth, the legend: Bill Murray; Ju...      wired   \n",
       "14872  A seafood allergy; a Penicillium allergy (no b...      wired   \n",
       "14936  So over the course of one strange week in San ...      wired   \n",
       "15399  Who gets the bigger room? Who gets to name the...      wired   \n",
       "15410  There are bad parts. Say you go to bed late, b...      wired   \n",
       "15430  Use a Kura bed as your frame and solid pine Iv...      wired   \n",
       "15669  But then I ask him how he feels about Lee’s de...      wired   \n",
       "16300  Start recording and put your phone down. If yo...      wired   \n",
       "16657  Pack everything you think you need, then get r...      wired   \n",
       "16840  Over the years, the increasingly complicated c...      wired   \n",
       "17090  Fridges will be useless once the grid gives ou...      wired   \n",
       "17177  NUM–APRIL NUM (AT LEAST) Targets_____ Westingh...      wired   \n",
       "...                                                  ...        ...   \n",
       "54534  Moonlight teens win MTV award for Best Kiss. I...         ew   \n",
       "54688  I got to the point where she pulls the boy's l...         ew   \n",
       "54730  Mike Myers wants a fourth Austin Powers movie,...         ew   \n",
       "54872  It was the kind of venue where you wafted. Fle...         ew   \n",
       "54881  I didn't think of anything beyond the play at ...         ew   \n",
       "54894  That's the best way to be. I have never felt l...         ew   \n",
       "54956  Superman. I tried out for the part in NUM for ...         ew   \n",
       "54958  The role has gotten more difficult over the la...         ew   \n",
       "55090  \"The songs tend to change quite a lot. I mean,...         ew   \n",
       "55529  Cloris came in like a whirlwind, all over the ...         ew   \n",
       "55534  I was concerned that if I did a nude scene it ...         ew   \n",
       "55542  Ben Johnson, man. He was so great. I got to dr...         ew   \n",
       "55548  I was mad that Peter only let me do that scene...         ew   \n",
       "55556  I didn't expect it at all. I got a dress and i...         ew   \n",
       "55656  Still, \"that was a very big day for me,\" Thoma...         ew   \n",
       "55718  I was on tour [in NUM] and we had a day off. W...         ew   \n",
       "55776  \"I START MAKING THE PHONE call. It rings. Made...         ew   \n",
       "55780  I was a bookworm. I ate books. I read so many,...         ew   \n",
       "55818  In that way, Five Came Back is not just a war ...         ew   \n",
       "55919  I am grateful—I never thought we would be here...         ew   \n",
       "56236  Thank you, Jack. I don't know why my husband l...         ew   \n",
       "56245  Jack does not want to take anybody out on Frid...         ew   \n",
       "56511  And with those words (from the ghost of her de...         ew   \n",
       "56683  When you dress me like this! And when you pose...         ew   \n",
       "56758  She's played twice. She's won twice. Sandra kn...         ew   \n",
       "57074  La La Land scored NUM noms. It will need to wi...         ew   \n",
       "57447  It was also painful for the actors, who had to...         ew   \n",
       "57572  No one will have to fight these two for a spot...         ew   \n",
       "57903  You move to L.A. To make ends meet, you take j...         ew   \n",
       "57904  Then one day, a miracle. You get your break. A...         ew   \n",
       "\n",
       "       readability_f                               Content_Preprocessed  \n",
       "2102          116.35  Vladimir frowns and thinks. And then he clicks...  \n",
       "6436          -46.27  The exhibition proceeds broadly chronologicall...  \n",
       "10091         100.58  Westron wynde, when wilt thou blow, the small ...  \n",
       "11424         101.09  [1] A tiny suction hose pulls out and holds th...  \n",
       "11508         101.60  They were in luck--the attendant gassed up the...  \n",
       "11668         101.60  [5] \"The water jug helps me track how much wat...  \n",
       "11819         102.10  Land quarries and riverbanks were once the mai...  \n",
       "11878         100.88  DREAD PIRATE ROBERTS 3/NUM/NUM 8:NUM Don't wan...  \n",
       "11998         104.44  Lucas: We had NUM shots to get through. They'd...  \n",
       "12044         102.10  Dippé: We made so much noise that they put us ...  \n",
       "12072         104.64  Kennedy: ILM did a proof-of-concept shot for T...  \n",
       "12082         102.10  Jaeger: I've worked on seven of his films now....  \n",
       "12393         103.12  DREAD: well, I'm also learning who I am. I don...  \n",
       "12653         105.35  What went wrong? I had used the best Belgian b...  \n",
       "12763         -19.54  Now here you are, on the cusp of carrying that...  \n",
       "12967         101.29  And so this issue of WIRED brings you NUM page...  \n",
       "13982         105.35  It's all up to you. Only you can save it. Only...  \n",
       "14338         -35.44  The secret burrito compartment in my car's das...  \n",
       "14803         -11.42  The man, the myth, the legend: Bill Murray; Ju...  \n",
       "14872         -36.80  A seafood allergy; a Penicillium allergy (no b...  \n",
       "14936         101.60  So over the course of one strange week in San ...  \n",
       "15399         103.42  Who gets the bigger room? Who gets to name the...  \n",
       "15410         105.66  There are bad parts. Say you go to bed late, b...  \n",
       "15430         100.58  Use a Kura bed as your frame and solid pine Iv...  \n",
       "15669         113.81  But then I ask him how he feels about Lee’s de...  \n",
       "16300         100.78  Start recording and put your phone down. If yo...  \n",
       "16657         100.88  Pack everything you think you need, then get r...  \n",
       "16840          -6.70  Over the years, the increasingly complicated c...  \n",
       "17090         102.10  Fridges will be useless once the grid gives ou...  \n",
       "17177          -0.26  NUM–APRIL NUM (AT LEAST) Targets_____ Westingh...  \n",
       "...              ...                                                ...  \n",
       "54534         102.10  Moonlight teens win MTV award for Best Kiss. I...  \n",
       "54688         102.10  I got to the point where she pulls the boy's l...  \n",
       "54730         101.09  Mike Myers wants a fourth Austin Powers movie,...  \n",
       "54872         107.69  It was the kind of venue where you wafted. Fle...  \n",
       "54881         102.31  I didn't think of anything beyond the play at ...  \n",
       "54894         100.58  That's the best way to be. I have never felt l...  \n",
       "54956         102.81  Superman. I tried out for the part in NUM for ...  \n",
       "54958         103.83  The role has gotten more difficult over the la...  \n",
       "55090         101.80  \"The songs tend to change quite a lot. I mean,...  \n",
       "55529         100.78  Cloris came in like a whirlwind, all over the ...  \n",
       "55534         108.23  I was concerned that if I did a nude scene it ...  \n",
       "55542         106.16  Ben Johnson, man. He was so great. I got to dr...  \n",
       "55548         103.83  I was mad that Peter only let me do that scene...  \n",
       "55556         103.63  I didn't expect it at all. I got a dress and i...  \n",
       "55656         103.32  Still, \"that was a very big day for me,\" Thoma...  \n",
       "55718         101.60  I was on tour [in NUM] and we had a day off. W...  \n",
       "55776         100.38  \"I START MAKING THE PHONE call. It rings. Made...  \n",
       "55780         105.96  I was a bookworm. I ate books. I read so many,...  \n",
       "55818         104.94  In that way, Five Came Back is not just a war ...  \n",
       "55919         102.91  I am grateful—I never thought we would be here...  \n",
       "56236         104.94  Thank you, Jack. I don't know why my husband l...  \n",
       "56245         106.67  Jack does not want to take anybody out on Frid...  \n",
       "56511         101.09  And with those words (from the ghost of her de...  \n",
       "56683         104.94  When you dress me like this! And when you pose...  \n",
       "56758         103.12  She's played twice. She's won twice. Sandra kn...  \n",
       "57074         102.10  La La Land scored NUM noms. It will need to wi...  \n",
       "57447         103.63  It was also painful for the actors, who had to...  \n",
       "57572         101.60  No one will have to fight these two for a spot...  \n",
       "57903         100.54  You move to L.A. To make ends meet, you take j...  \n",
       "57904         108.40  Then one day, a miracle. You get your break. A...  \n",
       "\n",
       "[230 rows x 4 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need to explore this futher. Brought it down to 230 from over 2.5k\n",
    "final_df1[(final_df1['readability_f']<0)|(final_df1['readability_f']>100) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def getMeanSentLen(para):\n",
    "    #Count number of sentences\n",
    "    sent_tok = nltk.sent_tokenize(para)\n",
    "    ns = len(sent_tok)\n",
    "    \n",
    "    #Count number of words\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    \n",
    "    ##Average Sentence length are words divided by sentences\n",
    "    avg=nw/ns\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def getSDSentLen(para):\n",
    "    #Count number of sentences\n",
    "    sent_tok = nltk.sent_tokenize(para)\n",
    "    ns = len(sent_tok)\n",
    "    \n",
    "    #Count number of words\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    \n",
    "    ##Average Sentence length are words divided by sentences\n",
    "    avg=nw/ns\n",
    "    #print(avg)\n",
    "    sum1=0\n",
    "    #Standard Deviation \n",
    "    for sent in sent_tok:\n",
    "        sum1=sum1+(len(sent.split())-avg)**2\n",
    "        #print(len(sent.split()))\n",
    "        #print(sum1)\n",
    "    sd=(sum1/ns)**0.5\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def normStopWordFrequency(para):\n",
    "    stopwords1=set(stopwords.words('english'))\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    word_tok_stop=[i for i in word_tok if i.lower() in stopwords1]\n",
    "    n_stop=len(word_tok_stop)\n",
    "    return(n_stop/nw)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def normFunctWordFrequency(functional,para):\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    word_tok_funct=[i for i in word_tok if i.lower() in functional]\n",
    "    n_funct=len(word_tok_funct)\n",
    "    return(n_funct/nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "def normPunctFrequency(para):\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "                               \n",
    "    no_punct = count(para, string.punctuation)\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    return(no_punct/nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38461538461538464"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normStopWordFrequency(final_df1.loc[0,'Content_Preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.25\n",
      "9\n",
      "52.5625\n",
      "26\n",
      "147.625\n",
      "19\n",
      "155.1875\n",
      "11\n",
      "182.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.7592529172978875"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSDSentLen(final_df1.loc[0,'Content_Preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#final_df1['avg_sent_len'],final_df1['sd_sent_len'] = final_df1['Content_Preprocessed'].apply(lambda x:getMeanSDSentLen(x))\n",
    "\n",
    "final_df1['avg_sent_len'] = final_df1.apply(lambda row: getMeanSentLen(row['Content_Preprocessed']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1['sd_sent_len'] = final_df1.apply(lambda row: getSDSentLen(row['Content_Preprocessed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1['norm_stop_freq'] = final_df1.apply(lambda row: normStopWordFrequency(row['Content_Preprocessed']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1['norm_punct_freq'] = final_df1.apply(lambda row: normPunctFrequency(row['Content_Preprocessed']), axis=1)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'via', 'whether', 'myself', 'that', 'either', 'your', 'I', 'of', 'be', 'about', 'consequently', 'been', 'without', 'by', 'besides', 'next', 'meanwhile', 'his', 'each', 'till', 'sure', 'itself', 'rather', 'he', 'will', 'ourselves', 'how', 'likewise', 'themselves', 'before', 'whatever', 'yeah', 'because', 'shall', 'to', 'every', 'him', 'during', 'our', 'quite', 'beyond', 'following', 'towards', 'instead', 'are', 'oh', 'nowhere', 'as', 'when', 'eight', 'many', 'against', 'whoever', 'than', 'my', 'opposite', 'me', 'whereas', 'excluding', 'one', 'them', 'last', 'five', 'neither', 'from', 'between', 'somewhere', 'whose', 'am', 'ok', 'through', 'okay', 'over', 'all', 'ought', 'ten', 'onto', 'thirty', 'toward', 'underneath', 'up', 'not', 'hence', 'other', 'yourself', 'regarding', 'except', 'little', 'unless', 'versus', 'was', 'whenever', 'any', 'could', 'those', 'lest', 'has', 'second', 'considering', 'its', 'were', 'course', 'then', 'inside', 'it', 'first', 'had', 'did', 'further', 'conversely', 'across', 'is', 'may', 'behind', 'would', 'otherwise', 'down', 'two', 'their', 'what', 'under', 'these', 'and', 'the', 'himself', 'her', 'whichever', 'even', 'having', 'at', 'upon', 'despite', 'once', 'they', 'why', 'a', 'yes', 'after', 'you', 'another', 'yours', 'should', 'an', 'moreover', 'thus', 'third', 'us', 'beside', 'both', 'for', 'lot', 'whomever', 'half', 'but', 'above', 'excepting', 'do', 'wherever', 'hardly', 'she', 'accordingly', 'does', 'since', 'most', 'in', 'done', 'below', 'must', 'mine', 'among', 'whom', 'enough', 'herself', 'only', 'nevertheless', 'ours', 'into', 'furthermore', 'being', 'therefore', 'seven', 'off', 'outside', 'within', 'four', 'although', 'hers', 'six', 'have', 'theirs', 'few', 'minus', 'until', 'plus', 'aboard', 'still', 'on', 'concerning', 'this', 'per', 'no', 'near', 'which', 'past', 'previous', 'such', 'with', 'beneath', 'like', 'might', 'much', 'we', 'amid', 'if', 'however', 'nine', 'though', 'round', 'unlike', 'nonetheless', 'three', 'some', 'anti', 'also', 'can', 'around', 'along', 'doing', 'while']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "functional_file = open(\"functional.txt\", \"r\")\n",
    "words= [word.strip() for line in functional_file.readlines() for word in line.split(',') if word.strip()]\n",
    "functional=list(set(words))\n",
    "print((functional))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1['norm_funct_freq'] = final_df1.apply(lambda row: normFunctWordFrequency(functional,row['Content_Preprocessed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "      <th>readability_f</th>\n",
       "      <th>Content_Preprocessed</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>sd_sent_len</th>\n",
       "      <th>norm_stop_freq</th>\n",
       "      <th>norm_punct_freq</th>\n",
       "      <th>readability_SMOG</th>\n",
       "      <th>norm_funct_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE first bicycles were freed on July NUMth, N...</td>\n",
       "      <td>economist</td>\n",
       "      <td>71.85</td>\n",
       "      <td>THE first bicycles were freed on July NUMth, N...</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>6.759253</td>\n",
       "      <td>0.446154</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.507692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roel van Duijn and Luud Schimmelpennink starte...</td>\n",
       "      <td>economist</td>\n",
       "      <td>83.25</td>\n",
       "      <td>Roel van Duijn and Luud Schimmelpennink starte...</td>\n",
       "      <td>13.428571</td>\n",
       "      <td>7.326218</td>\n",
       "      <td>0.457447</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.457447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A few days later, at a street meeting where Mr...</td>\n",
       "      <td>economist</td>\n",
       "      <td>83.46</td>\n",
       "      <td>A few days later, at a street meeting where Mr...</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>8.885944</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Half a century later the streets of Beijing ar...</td>\n",
       "      <td>economist</td>\n",
       "      <td>79.30</td>\n",
       "      <td>Half a century later the streets of Beijing ar...</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>6.952218</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.509434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The yellow bikes are from Ofo, so named becaus...</td>\n",
       "      <td>economist</td>\n",
       "      <td>72.16</td>\n",
       "      <td>The yellow bikes are from Ofo, so named becaus...</td>\n",
       "      <td>16.166667</td>\n",
       "      <td>3.113590</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.402062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content     source  \\\n",
       "0  THE first bicycles were freed on July NUMth, N...  economist   \n",
       "1  Roel van Duijn and Luud Schimmelpennink starte...  economist   \n",
       "2  A few days later, at a street meeting where Mr...  economist   \n",
       "3  Half a century later the streets of Beijing ar...  economist   \n",
       "4  The yellow bikes are from Ofo, so named becaus...  economist   \n",
       "\n",
       "   readability_f                               Content_Preprocessed  \\\n",
       "0          71.85  THE first bicycles were freed on July NUMth, N...   \n",
       "1          83.25  Roel van Duijn and Luud Schimmelpennink starte...   \n",
       "2          83.46  A few days later, at a street meeting where Mr...   \n",
       "3          79.30  Half a century later the streets of Beijing ar...   \n",
       "4          72.16  The yellow bikes are from Ofo, so named becaus...   \n",
       "\n",
       "   avg_sent_len  sd_sent_len  norm_stop_freq  norm_punct_freq  \\\n",
       "0     16.250000     6.759253        0.446154         0.184615   \n",
       "1     13.428571     7.326218        0.457447         0.223404   \n",
       "2     13.200000     8.885944        0.424242         0.136364   \n",
       "3     17.666667     6.952218        0.509434         0.207547   \n",
       "4     16.166667     3.113590        0.371134         0.175258   \n",
       "\n",
       "   readability_SMOG  norm_funct_freq  \n",
       "0              10.1         0.507692  \n",
       "1               9.6         0.457447  \n",
       "2               7.6         0.393939  \n",
       "3               6.4         0.509434  \n",
       "4               9.3         0.402062  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features= final_df1[['source','readability_f','readability_SMOG','avg_sent_len','sd_sent_len','norm_stop_freq','norm_punct_freq','norm_funct_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>readability_f</th>\n",
       "      <th>readability_SMOG</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>sd_sent_len</th>\n",
       "      <th>norm_stop_freq</th>\n",
       "      <th>norm_punct_freq</th>\n",
       "      <th>norm_funct_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>economist</td>\n",
       "      <td>71.85</td>\n",
       "      <td>10.1</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>6.759253</td>\n",
       "      <td>0.446154</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.507692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economist</td>\n",
       "      <td>83.25</td>\n",
       "      <td>9.6</td>\n",
       "      <td>13.428571</td>\n",
       "      <td>7.326218</td>\n",
       "      <td>0.457447</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>0.457447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>economist</td>\n",
       "      <td>83.46</td>\n",
       "      <td>7.6</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>8.885944</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>economist</td>\n",
       "      <td>79.30</td>\n",
       "      <td>6.4</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>6.952218</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.509434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economist</td>\n",
       "      <td>72.16</td>\n",
       "      <td>9.3</td>\n",
       "      <td>16.166667</td>\n",
       "      <td>3.113590</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.402062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source  readability_f  readability_SMOG  avg_sent_len  sd_sent_len  \\\n",
       "0  economist          71.85              10.1     16.250000     6.759253   \n",
       "1  economist          83.25               9.6     13.428571     7.326218   \n",
       "2  economist          83.46               7.6     13.200000     8.885944   \n",
       "3  economist          79.30               6.4     17.666667     6.952218   \n",
       "4  economist          72.16               9.3     16.166667     3.113590   \n",
       "\n",
       "   norm_stop_freq  norm_punct_freq  norm_funct_freq  \n",
       "0        0.446154         0.184615         0.507692  \n",
       "1        0.457447         0.223404         0.457447  \n",
       "2        0.424242         0.136364         0.393939  \n",
       "3        0.509434         0.207547         0.509434  \n",
       "4        0.371134         0.175258         0.402062  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X -> features, y -> label\n",
    "X=df_features[['readability_f','readability_SMOG','avg_sent_len','sd_sent_len','norm_stop_freq','norm_punct_freq','norm_funct_freq']]\n",
    "y=df_features.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing X, y into train and test data \n",
    "#We use stratified sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "svm_predictions = svm_model_linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525672206459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "accuracy = svm_model_linear.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1991  352  841  117]\n",
      " [ 423 1846 1548  191]\n",
      " [ 709  605 3650   60]\n",
      " [ 833  671  671  294]]\n"
     ]
    }
   ],
   "source": [
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, svm_predictions)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
