{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import itertools\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import csv\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk import pos_tag\n",
    "from nltk.tag.mapping import tagset_mapping\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(path, replace_dict, min_length=10):\n",
    "    \"\"\"\n",
    "    Removes source-specific artifacts from documents.\n",
    "    \n",
    "    :param path: filepath containing JSON files for each document\n",
    "    :param replace_dict: dictionary containing regex matching strings and strings to replace them with\n",
    "    :param min_length: minimum document length\n",
    "    \n",
    "    :returns: list of strings, each containing document content\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = []\n",
    "    #files = os.listdir(path)\n",
    "    json_files = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n",
    "\n",
    "    for file in json_files:\n",
    "        #file = '\\\\' + file\n",
    "        content = json.load(open(path + file))['content']\n",
    "        \n",
    "        # replace regex strings\n",
    "        for key, value in replace_dict.items():\n",
    "            content = re.sub(key, value, content)\n",
    "        \n",
    "        # remove small documents\n",
    "        if len(content) >= min_length:\n",
    "            docs.append(content)\n",
    "        \n",
    "    return(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "economist_path = 'data_updated/economist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "economist_dict = {}\n",
    "\n",
    "# artifacts on accented letters\n",
    "economist_dict['AaAaAeA '] = 'i'\n",
    "economist_dict['AaAaAeAo'] = 'c'\n",
    "economist_dict['AaAaAeAc'] = 'a'\n",
    "economist_dict['AaAaAeA~'] = 'n'\n",
    "economist_dict['AaAaAeA@|AaAaAeA\\?|AaAaAeA{|AaAaAe'] = 'e'\n",
    "\n",
    "\n",
    "\n",
    "# numbers\n",
    "economist_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "\n",
    "# Go online artifacts\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "economist_dict['<p>Go online ([^<]*)</p>|<p>([^<]*)([^.?!\"]){1}</p>'] = ''\n",
    "\n",
    "# end of paragraph tags\n",
    "economist_dict['</p>'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "economist_docs = process_corpus(economist_path, economist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104\n"
     ]
    }
   ],
   "source": [
    "print(len(economist_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "economist_paragraphs = []\n",
    "for doc in economist_docs:\n",
    "    economist_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "economist_paragraphs = [doc.strip() for doc in economist_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11198"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(economist_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wired_path = 'data_updated/wired/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wired_dict = {}\n",
    "\n",
    "# numbers\n",
    "wired_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "# author/subject descriptions at end of article\n",
    "# paragraph symbols\n",
    "wired_dict['<p>([^<]*)([^.?!\"]){1}</p>|<p>([^<]*)([A-Z]+) \\(@(.*)|Â¶'] = ''\n",
    "\n",
    "#email addresses\n",
    "wired_dict['[\\w\\.-]+@[\\w\\.-]+']=''\n",
    "\n",
    "# end of paragraph tags\n",
    "wired_dict['</p>'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wired_docs = process_corpus(wired_path, wired_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296\n"
     ]
    }
   ],
   "source": [
    "print(len(wired_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wired_paragraphs = []\n",
    "\n",
    "for doc in wired_docs:\n",
    "    wired_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "wired_paragraphs = [doc.strip() for doc in wired_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17142"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wired_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Yorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyorker_path = 'data_updated/newyorker/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyorker_dict = {}\n",
    "\n",
    "# artifacts on accented letters\n",
    "newyorker_dict['AaAaAeA '] = 'i'\n",
    "newyorker_dict['AaAaAeAo'] = 'c'\n",
    "newyorker_dict['AaAaAeAc'] = 'a'\n",
    "newyorker_dict['AaAaAeA~'] = 'n'\n",
    "newyorker_dict['AaAaAeA@|AaAaAeA\\?|AaAaAeA{|AaAaAe'] = 'e'\n",
    "\n",
    "# numbers\n",
    "newyorker_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "# bylines\n",
    "newyorker_dict['<p>([^<]*)([^.?!\"]){1}</p>|<p>Byline([^<]*)</p>'] = ''\n",
    "\n",
    "# end of paragraph tags\n",
    "newyorker_dict['</p>'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyorker_docs = process_corpus(newyorker_path, newyorker_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807\n"
     ]
    }
   ],
   "source": [
    "print(len(newyorker_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyorker_paragraphs = []\n",
    "for doc in newyorker_docs:\n",
    "    newyorker_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "newyorker_paragraphs = [doc.strip() for doc in newyorker_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19030"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newyorker_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_path = 'data_updated/ew/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_dict = {}\n",
    "\n",
    "# artifacts on accented letters\n",
    "ew_dict['AaAaAeA '] = 'i'\n",
    "ew_dict['AaAaAeAo'] = 'c'\n",
    "ew_dict['AaAaAeAc'] = 'a'\n",
    "ew_dict['AaAaAeA~'] = 'n'\n",
    "ew_dict['AaAaAeA@|AaAaAeA\\?|AaAaAeA{|AaAaAe'] = 'e'\n",
    "\n",
    "# numbers\n",
    "ew_dict['([\\d]+)([.,]?)([\\d]+)'] = 'NUM'\n",
    "\n",
    "#email addresses\n",
    "ew_dict['[\\w\\.-]+@[\\w\\.-]+']=''\n",
    "\n",
    "# Go online artifacts\n",
    "# end paragraph without punctuation (probably headers or titles)\n",
    "#ew_dict['<p>Go online ([^<]*)</p>|<p>([^<]*)([^.?!+\"]){1}</p>'] = ''\n",
    "\n",
    "#bullet points\n",
    "ew_dict['\\xc2\\xb7']=''\n",
    "\n",
    "# end of paragraph tags\n",
    "ew_dict['</p>'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_docs = process_corpus(ew_path, ew_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2190\n"
     ]
    }
   ],
   "source": [
    "print(len(ew_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_paragraphs = []\n",
    "\n",
    "for doc in ew_docs:\n",
    "    ew_paragraphs += doc.strip().split('<p>')\n",
    "    \n",
    "ew_paragraphs = [doc.strip() for doc in ew_paragraphs if len(doc) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ratings(graf):\n",
    "    \n",
    "    \"\"\"Removes ratings such as \"B+\", \"A\", \"C-\" from the end of a paragraph.\"\"\"\n",
    "    \n",
    "    # Check if the last character in the last word is a sentence-ending character ('.', '!' and so on)\n",
    "    if not any(x == graf.split()[-1][-1] for x in ['.', '!', '?', '\\\"', ')', 'â', 'â¦']):\n",
    "        \n",
    "        # If not, return paragraph with the last word removed\n",
    "        return graf[0:-len(graf.split()[-1])].strip()\n",
    "    \n",
    "    else:\n",
    "        return graf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_paragraphs = [remove_ratings(p) for p in ew_paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17536\n"
     ]
    }
   ],
   "source": [
    "print(len(ew_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = economist_paragraphs + wired_paragraphs + newyorker_paragraphs+ew_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = list(itertools.repeat('economist', len(economist_paragraphs)))\n",
    "sources += list(itertools.repeat('wired', len(wired_paragraphs)))\n",
    "sources += list(itertools.repeat('newyorker', len(newyorker_paragraphs)))\n",
    "sources +=list(itertools.repeat('ew',len(ew_paragraphs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'content':paragraphs, 'source':sources}\n",
    "final_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64906"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOWN the Euphrates river, halfway between Deir...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Never have America and its allies had such a h...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But like their Parthian forebears, Iran and it...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iran's gains are even more striking elsewhere....</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Farther south, America's hopes of stemming Ira...</td>\n",
       "      <td>economist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content     source\n",
       "0  DOWN the Euphrates river, halfway between Deir...  economist\n",
       "1  Never have America and its allies had such a h...  economist\n",
       "2  But like their Parthian forebears, Iran and it...  economist\n",
       "3  Iran's gains are even more striking elsewhere....  economist\n",
       "4  Farther south, America's hopes of stemming Ira...  economist"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.to_csv('pre_pre_processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHTMLTags(x):\n",
    "    content = re.sub(\"(?i)<\\/?\\w+((\\s+\\w+(\\s*=\\s*(?:\\\".*?\\\"|'.*?'|[^'\\\">\\s]+))?)+\\s*|\\s*)\\/?>\", '', x)\n",
    "    #content1 = re.sub(\"\\\\b\\\\x94\\\\b\", ' ', content)\n",
    "    \n",
    "    return(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Content_Preprocessed']=final_df['content'].apply(lambda x:removeHTMLTags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    DOWN the Euphrates river, halfway between Deir...\n",
       "1    Never have America and its allies had such a h...\n",
       "2    But like their Parthian forebears, Iran and it...\n",
       "3    Iran's gains are even more striking elsewhere....\n",
       "4    Farther south, America's hopes of stemming Ira...\n",
       "Name: Content_Preprocessed, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['Content_Preprocessed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a version of the content where all text inside quotes is replacde by 'Qx'\n",
    "\n",
    "def replace_quotes(X):\n",
    "    \n",
    "    \"\"\"\n",
    "    Replaces all text inside quotes with 'Qx'.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = re.sub(r'\\\"([^\\\"]+?)[^!?\\'\\.,]\\\" ', \"\\'Qx\\' \", X) # \"text\"+space\n",
    "    \n",
    "    X = re.sub(r'\\\"([^\\\"]+?)\\\"\\.', \"\\'Qx\\'.\", X) # \"text\". -- applies to economist (british)\n",
    "    X = re.sub(r'\\\"([^\\\"]+?)\\\",', \"\\'Qx\\',\", X) # \"text\", -- applies to economist (british)\n",
    "    \n",
    "    X = re.sub(r'\\\"([^\\\"]+?),\\\"', \"\\'Qx,\\'\", X) # \"text,\"   \n",
    "    X = re.sub(r'\\\"([^\\\"]+?)[!?\\'\\.]\\\"', \"\\'Qx.\\'\", X) # \"text!\" or \"text?\" or \"text.\" \n",
    "    \n",
    "    X = re.sub(r'â(.+?),â', \"\\'Qx,\\'\", X) # âtext,â (CURLY QUOTES)\n",
    "    X = re.sub(r'â(.+?)[! ?\\'\\.]â', \"\\'Qx.\\'\", X) # âtext!â or âtext?â or âtext.â\n",
    "    X = re.sub(r'â(.+?)â', \"\\'Qx\\'\", X) # âtextâ\n",
    "\n",
    "    X = re.sub(r'\\x93(.+?),\\x94', \"\\'Qx,\\'\", X) # \"text,\" -- applies to wired (weird chars)\n",
    "    X = re.sub(r'\\x93(.+?)[! ?\\'\\.]\\x94', \"\\'Qx.\\'\", X) # \"text!\" or \"text \" or \"text?\" or \"text.\" -- wired\n",
    "    X = re.sub(r'\\x93(.+?)\\x94', \"\\'Qx\\'\", X) # \"text\" -- applies to wired (weird chars)\n",
    "    \n",
    "    return X\n",
    "\n",
    "replaced_text = [replace_quotes(s) for s in final_df['Content_Preprocessed'].values]\n",
    "\n",
    "final_df['Content_NoQuotes'] = replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out paragraphs with 20 non-quote words or less\n",
    "\n",
    "# final_df_filtered = final_df[final_df['Content_NoQuotes'].apply(lambda x: len([w for w in x.split() if 'Qx' not in w])) > 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_bool = final_df['Content_NoQuotes'].apply(lambda x: len([w for w in x.split() if 'Qx' not in w])) > 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered = final_df[['source', 'Content_Preprocessed', 'Content_NoQuotes']][filter_bool.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52812, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_Preprocessed</th>\n",
       "      <th>Content_NoQuotes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>economist</th>\n",
       "      <td>10982</td>\n",
       "      <td>10982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ew</th>\n",
       "      <td>12414</td>\n",
       "      <td>12414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newyorker</th>\n",
       "      <td>15780</td>\n",
       "      <td>15780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wired</th>\n",
       "      <td>13636</td>\n",
       "      <td>13636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Content_Preprocessed  Content_NoQuotes\n",
       "source                                           \n",
       "economist                 10982             10982\n",
       "ew                        12414             12414\n",
       "newyorker                 15780             15780\n",
       "wired                     13636             13636"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class balance\n",
    "\n",
    "final_df_filtered.groupby('source').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability scores\n",
    "# final_df_filtered['readability_f'] = final_df_filtered['Content_Preprocessed'].apply(textstat.flesch_reading_ease)\n",
    "final_df_filtered['readability_SMOG']=final_df_filtered['Content_Preprocessed'].apply(textstat.smog_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by sentences\n",
    "final_df_filtered['Sentence_Tokens'] = final_df_filtered['Content_Preprocessed'].apply(nltk.sent_tokenize)\n",
    "\n",
    "# Number of sentences = length of sentence tokens list\n",
    "final_df_filtered['Num_Sentences'] = final_df_filtered['Sentence_Tokens'].apply(len)\n",
    "\n",
    "# Average sentence length = number of words/number of sentences\n",
    "word_counts = final_df_filtered['Content_Preprocessed'].apply(lambda x: len(x.split()))\n",
    "final_df_filtered['avg_sent_len'] = word_counts/final_df_filtered['Num_Sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>source</th>\n",
       "      <th>Content_Preprocessed</th>\n",
       "      <th>Content_NoQuotes</th>\n",
       "      <th>readability_SMOG</th>\n",
       "      <th>Sentence_Tokens</th>\n",
       "      <th>Num_Sentences</th>\n",
       "      <th>avg_sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19400</th>\n",
       "      <td>21553</td>\n",
       "      <td>wired</td>\n",
       "      <td>WIRED: Peter Thiel, expressing his dissatisfac...</td>\n",
       "      <td>WIRED: Peter Thiel, expressing his dissatisfac...</td>\n",
       "      <td>10.6</td>\n",
       "      <td>[WIRED: Peter Thiel, expressing his dissatisfa...</td>\n",
       "      <td>38</td>\n",
       "      <td>13.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18741</th>\n",
       "      <td>20796</td>\n",
       "      <td>wired</td>\n",
       "      <td>WIRED: You founded Metafilter, a communal webl...</td>\n",
       "      <td>WIRED: You founded Metafilter, a communal webl...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[WIRED: You founded Metafilter, a communal web...</td>\n",
       "      <td>37</td>\n",
       "      <td>13.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38377</th>\n",
       "      <td>44973</td>\n",
       "      <td>newyorker</td>\n",
       "      <td>In a conversation that took place in a buildin...</td>\n",
       "      <td>In a conversation that took place in a buildin...</td>\n",
       "      <td>10.7</td>\n",
       "      <td>[In a conversation that took place in a buildi...</td>\n",
       "      <td>36</td>\n",
       "      <td>3.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40780</th>\n",
       "      <td>47907</td>\n",
       "      <td>ew</td>\n",
       "      <td>YOU KNOW IT'S BEEN A GOOD PARTY WHEN JOSS WHED...</td>\n",
       "      <td>YOU KNOW IT'S BEEN A GOOD PARTY WHEN JOSS WHED...</td>\n",
       "      <td>10.6</td>\n",
       "      <td>[YOU KNOW IT'S BEEN A GOOD PARTY WHEN JOSS WHE...</td>\n",
       "      <td>33</td>\n",
       "      <td>16.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15399</th>\n",
       "      <td>16615</td>\n",
       "      <td>wired</td>\n",
       "      <td>GEORGE CARLIN. Lenny Bruce. That dog puppet. W...</td>\n",
       "      <td>GEORGE CARLIN. Lenny Bruce. That dog puppet. W...</td>\n",
       "      <td>10.3</td>\n",
       "      <td>[GEORGE CARLIN., Lenny Bruce., That dog puppet...</td>\n",
       "      <td>31</td>\n",
       "      <td>12.451613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index     source                               Content_Preprocessed  \\\n",
       "19400  21553      wired  WIRED: Peter Thiel, expressing his dissatisfac...   \n",
       "18741  20796      wired  WIRED: You founded Metafilter, a communal webl...   \n",
       "38377  44973  newyorker  In a conversation that took place in a buildin...   \n",
       "40780  47907         ew  YOU KNOW IT'S BEEN A GOOD PARTY WHEN JOSS WHED...   \n",
       "15399  16615      wired  GEORGE CARLIN. Lenny Bruce. That dog puppet. W...   \n",
       "\n",
       "                                        Content_NoQuotes  readability_SMOG  \\\n",
       "19400  WIRED: Peter Thiel, expressing his dissatisfac...              10.6   \n",
       "18741  WIRED: You founded Metafilter, a communal webl...               9.0   \n",
       "38377  In a conversation that took place in a buildin...              10.7   \n",
       "40780  YOU KNOW IT'S BEEN A GOOD PARTY WHEN JOSS WHED...              10.6   \n",
       "15399  GEORGE CARLIN. Lenny Bruce. That dog puppet. W...              10.3   \n",
       "\n",
       "                                         Sentence_Tokens  Num_Sentences  \\\n",
       "19400  [WIRED: Peter Thiel, expressing his dissatisfa...             38   \n",
       "18741  [WIRED: You founded Metafilter, a communal web...             37   \n",
       "38377  [In a conversation that took place in a buildi...             36   \n",
       "40780  [YOU KNOW IT'S BEEN A GOOD PARTY WHEN JOSS WHE...             33   \n",
       "15399  [GEORGE CARLIN., Lenny Bruce., That dog puppet...             31   \n",
       "\n",
       "       avg_sent_len  \n",
       "19400     13.421053  \n",
       "18741     13.027027  \n",
       "38377      3.416667  \n",
       "40780     16.515152  \n",
       "15399     12.451613  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_filtered.sort_values(by = 'Num_Sentences', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52812.000000\n",
       "mean         4.953912\n",
       "std          2.728608\n",
       "min          1.000000\n",
       "25%          3.000000\n",
       "50%          5.000000\n",
       "75%          6.000000\n",
       "max         38.000000\n",
       "Name: Num_Sentences, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_filtered.Num_Sentences.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52812.000000\n",
       "mean        20.059174\n",
       "std          7.261715\n",
       "min          3.000000\n",
       "25%         15.300000\n",
       "50%         19.000000\n",
       "75%         23.500000\n",
       "max        180.000000\n",
       "Name: avg_sent_len, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_filtered.avg_sent_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard deviation sentence length for each paragraph\n",
    "\n",
    "def getSDSentLen(p):\n",
    "    sent_tokens = p[0]\n",
    "    avg_sent_len = p[1]\n",
    "    \n",
    "    # Get length of each sentence as np array\n",
    "    lengths = np.array([len(s.split()) for s in sent_tokens])\n",
    "    \n",
    "    # Calculate SD\n",
    "    return (np.sqrt(np.sum((lengths - avg_sent_len)**2)/len(sent_tokens)))\n",
    "\n",
    "final_df_filtered['sd_sent_len'] = final_df_filtered[['Sentence_Tokens',\n",
    "                                                      'avg_sent_len']].apply(lambda x: getSDSentLen(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If have time: These functions could be rewritten so they run faster...\n",
    "# decide what's not a word based on presence of alphabetic characters, not list of non-words\n",
    "\n",
    "def normStopWordFrequency(para):\n",
    "    stopwords1=set(stopwords.words('english'))\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\", \"'Qx\", \"'\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    word_tok_stop=[i for i in word_tok if i.lower() in stopwords1]\n",
    "    n_stop=len(word_tok_stop)\n",
    "    return(n_stop/nw)\n",
    "\n",
    "def normFunctWordFrequency(functional,para):\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\", \"'Qx\", \"'\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    word_tok_funct=[i for i in word_tok if i.lower() in functional]\n",
    "    n_funct=len(word_tok_funct)\n",
    "    return(n_funct/nw)\n",
    "\n",
    "def normPunctFrequency(para):\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "                               \n",
    "    no_punct = count(para, string.punctuation)\n",
    "    word_tok = nltk.word_tokenize(para) #need to take out commas plus other stuff\n",
    "    NoWord = [',','(',')',':',';','.','%','\\x96','\\x94','{','}','[',']','!','?',\"''\",\"``\", \"'Qx\", \"'\"]\n",
    "    word_tok2 = [i for i in word_tok if i not in NoWord]\n",
    "    nw = len(word_tok2)\n",
    "    return(no_punct/nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered['norm_stop_freq'] = final_df_filtered.apply(lambda row: normStopWordFrequency(row['Content_NoQuotes']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered['norm_punct_freq'] = final_df_filtered.apply(lambda row: normPunctFrequency(row['Content_NoQuotes']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get functional words\n",
    "\n",
    "functional_file = open(\"functional.txt\", \"r\")\n",
    "f_words = [word.strip() for line in functional_file.readlines() for word in line.split(',') if word.strip()]\n",
    "functional_file.close()\n",
    "\n",
    "functional = list(set(f_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_filtered['norm_funct_freq'] = final_df_filtered.apply(lambda row: normFunctWordFrequency(functional,\n",
    "                                                                                                  row['Content_NoQuotes']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tag/Punctuation-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF MOVING THIS UP, remember token is now \"Qx\" not \"'Qx\"\n",
    "\n",
    "def replace_qx(graf):\n",
    "    \n",
    "    \"\"\"\n",
    "    Replaces all instances of 'Qx' with \"Qx\"\n",
    "    \"\"\"\n",
    "\n",
    "    graf = re.sub(\"'Qx'\", '\"Qx\"', graf)\n",
    "    graf = re.sub(\"'Qx,'\", '\"Qx,\"', graf)\n",
    "    graf = re.sub(\"'Qx.'\", '\"Qx.\"', graf)\n",
    "    \n",
    "    return graf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Qx' with \"Qx\" so as to not confuse tokenizer/tagger \n",
    "replaced_qx = [replace_qx(p) for p in final_df_filtered['Content_NoQuotes'].values]\n",
    "\n",
    "final_df_filtered['Content_NoQuotes'] = replaced_qx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize paragraphs (with quotes rendered as \"Qx\")\n",
    "tokens = [nltk.word_tokenize(p) for p in final_df_filtered['Content_NoQuotes']]\n",
    "\n",
    "# get (word, tag) tuples\n",
    "pos_tags_raw = [pos_tag(token_list) for token_list in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_map = tagset_mapping('en-ptb', 'universal')\n",
    "\n",
    "def map_taglist(tag_list):\n",
    "    \"\"\"\n",
    "    Converts a list of PTB tags to a list of universal tags.\n",
    "    \"\"\"\n",
    "    return([universal_map[tag] for tag in tag_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = []\n",
    "\n",
    "# only keep actual tags, as well as 'Qx', '!' and '?', etc.\n",
    "for i in range(len(pos_tags_raw)):\n",
    "    pos_tags.append([p if t not in (\"Qx\",'!', '?', ';', ':') else t for t, p in pos_tags_raw[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get universal POS tags\n",
    "\n",
    "universal_tags = [map_taglist(taglist) for taglist in pos_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding tags back in to create custom tagset\n",
    "\n",
    "# ':' includes dashes and colons and semi-colons\n",
    "\n",
    "# list of accepted tags that we want to put back in\n",
    "# UH = interjection\n",
    "# VBG = gerund of present participle\n",
    "# VBD = Verb, past tense\n",
    "# PRP = Personal pronoun\n",
    "# VBN = Verb, past participle\n",
    "\n",
    "accepted_tags = (\"Qx\", ',', 'UH', 'VBG', 'VBD', 'PRP', 'NNS', 'VBP',\n",
    "                 '!', '?', '``', \"''\", ':', ';')\n",
    "\n",
    "def replace_tags(pos, univ):\n",
    "    \"\"\"Replaces tag in in universal list wherever it appears in POS tag list and is acceptable.\"\"\"\n",
    "    \n",
    "    for i in range(len(pos)):\n",
    "        if pos[i] in accepted_tags:\n",
    "            univ[i] = pos[i]\n",
    "    return univ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace tags\n",
    "universal_modified = [replace_tags(p, u) for p, u in list(zip(pos_tags,\n",
    "                                                              universal_tags))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put POS tags column in DF\n",
    "final_df_filtered['pos_tags'] = universal_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADJ', 'PRON', 'DET', 'VBG', 'VBD', 'PRP', 'X', 'VERB', ':', 'NUM', '.', 'NNS', 'CONJ', 'VBP', 'PRT', '?', \"''\", 'UH', ';', '!', 'ADP', 'NOUN', 'ADV', 'Qx', ',', '``'}\n"
     ]
    }
   ],
   "source": [
    "# See final tag set\n",
    "flat_tags = list(itertools.chain.from_iterable(final_df_filtered['pos_tags']))\n",
    "universal_tagset = set(flat_tags)\n",
    "print(universal_tagset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tag N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make bigrams and trigrams out of custom tagset\n",
    "\n",
    "final_df_filtered['tag_bigrams'] = final_df_filtered['pos_tags'].apply(lambda x: list(ngrams(x, 2)))\n",
    "final_df_filtered['tag_trigrams'] = final_df_filtered['pos_tags'].apply(lambda x: list(ngrams(x, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get set of all n-grams\n",
    "\n",
    "set_bigrams = set(list(itertools.chain.from_iterable(final_df_filtered['tag_bigrams'])))\n",
    "set_trigrams = set(list(itertools.chain.from_iterable(final_df_filtered['tag_trigrams'])))\n",
    "\n",
    "set_grams = list(set_bigrams) + list(set_trigrams) + list(universal_tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9844"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_set(p):\n",
    "    \n",
    "    \"\"\"Gets the set of unigrams, bigrams and trigrams in a paragraph.\"\"\"\n",
    "    \n",
    "    return set(p[0] + p[1] + p[2])\n",
    "\n",
    "\n",
    "grafs_set_grams = final_df_filtered[['pos_tags',\n",
    "                                     'tag_bigrams',\n",
    "                                     'tag_trigrams']].apply(lambda x: get_gram_set(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document frequency of each n-gram:\n",
    "\n",
    "# Flatten list so we can count the document frequency for each gram\n",
    "all_set_grams = [gram for p in grafs_set_grams for gram in p]\n",
    "\n",
    "# Create document frequency counter\n",
    "doc_freq = Counter(all_set_grams)\n",
    "\n",
    "# rank by count\n",
    "doc_freq_ranked = doc_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 52804),\n",
       " ('.', 52742),\n",
       " ('ADP', 52651),\n",
       " ('DET', 52531),\n",
       " ('VERB', 51666),\n",
       " ('ADJ', 51656),\n",
       " (',', 50897),\n",
       " (('DET', 'NOUN'), 50679),\n",
       " ('NNS', 48873),\n",
       " (('NOUN', 'ADP'), 48473),\n",
       " ('ADV', 48346),\n",
       " ('PRT', 48179),\n",
       " (('NOUN', '.'), 48130),\n",
       " (('ADJ', 'NOUN'), 47951),\n",
       " (('ADP', 'DET'), 47895),\n",
       " (('NOUN', 'NOUN'), 47119),\n",
       " ('CONJ', 46459),\n",
       " (('NOUN', ','), 46120),\n",
       " (('ADP', 'NOUN'), 46061),\n",
       " ('PRP', 44069),\n",
       " (('DET', 'ADJ'), 43619),\n",
       " (('ADP', 'DET', 'NOUN'), 41903),\n",
       " ('VBD', 39682),\n",
       " ('VBG', 39664),\n",
       " ('PRON', 39382),\n",
       " (('DET', 'ADJ', 'NOUN'), 38863),\n",
       " (('NOUN', 'VERB'), 35954),\n",
       " (('NOUN', 'PRT'), 35615),\n",
       " (('VERB', 'ADP'), 35411),\n",
       " (('DET', 'NOUN', 'ADP'), 35140)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See top 30 most common grams\n",
    "doc_freq_ranked[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document frequency of each n-gram grouped by publication\n",
    "\n",
    "# Get set of n-grams for every paragraph for every publication\n",
    "ec_set_grams = [grams for grams, in_ec in list(zip(grafs_set_grams,\n",
    "                                                   final_df_filtered['source']=='economist')) if in_ec]\n",
    "\n",
    "ny_set_grams = [grams for grams, in_ny in list(zip(grafs_set_grams,\n",
    "                                                   final_df_filtered['source']=='newyorker')) if in_ny]\n",
    "\n",
    "wd_set_grams = [grams for grams, in_wd in list(zip(grafs_set_grams,\n",
    "                                                   final_df_filtered['source']=='wired')) if in_wd]\n",
    "\n",
    "ew_set_grams = [grams for grams, in_ew in list(zip(grafs_set_grams,\n",
    "                                                   final_df_filtered['source']=='ew')) if in_ew]\n",
    "\n",
    "# Flatten lists so we can count grams and make counters\n",
    "ec_counter = Counter(list(itertools.chain.from_iterable(ec_set_grams)))\n",
    "ny_counter = Counter(list(itertools.chain.from_iterable(ny_set_grams)))\n",
    "wd_counter = Counter(list(itertools.chain.from_iterable(wd_set_grams)))\n",
    "ew_counter = Counter(list(itertools.chain.from_iterable(ew_set_grams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25819889419071423 0.20794516397788382 0.298795728243581 0.23506021358782095\n"
     ]
    }
   ],
   "source": [
    "# Get probability of each source\n",
    "\n",
    "source_counts = Counter(final_df_filtered['source'])\n",
    "total_grafs = len(final_df_filtered['source'])\n",
    "\n",
    "\n",
    "# Probability of seeing a WIRED paragraph\n",
    "p_wd = source_counts['wired']/total_grafs\n",
    "\n",
    "# Probability of seeing an Economist paragraph\n",
    "p_ec = source_counts['economist']/total_grafs\n",
    "\n",
    "# Probability of seeing a New Yorker paragraph\n",
    "p_ny = source_counts['newyorker']/total_grafs\n",
    "\n",
    "# Probability of seeing a WIRED paragraph\n",
    "p_ew = source_counts['ew']/total_grafs\n",
    "\n",
    "print(p_wd, p_ec, p_ny, p_ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wd': 0.25819889419071423,\n",
       " 'ec': 0.20794516397788382,\n",
       " 'ny': 0.298795728243581,\n",
       " 'ew': 0.23506021358782095}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary of source probabilities\n",
    "\n",
    "source_prob = dict(zip(['wd', 'ec', 'ny', 'ew'], [p_wd, p_ec, p_ny, p_ew]))\n",
    "\n",
    "source_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get source_counter[gram] for each source as a dictionary, for each gram\n",
    "\n",
    "def get_gram_dict(gram):\n",
    "    return dict(zip(['wd', 'ec', 'ny', 'ew'], [wd_counter[gram], ec_counter[gram], ny_counter[gram], ew_counter[gram]]))\n",
    "\n",
    "all_gram_dicts = dict(zip(set_grams, [get_gram_dict(gram) for gram in set_grams]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wd': 13633, 'ec': 10981, 'ny': 15780, 'ew': 12410}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gram_dict('NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wd': 13633, 'ec': 10981, 'ny': 15780, 'ew': 12410}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_gram_dicts['NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ew': 12414, 'wd': 13636, 'ny': 15780, 'ec': 10982})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update source_count keys\n",
    "source_counts['wd'] = source_counts.pop('wired')\n",
    "source_counts['ny'] = source_counts.pop('newyorker')\n",
    "source_counts['ec'] = source_counts.pop('economist')\n",
    "\n",
    "source_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ig(gram_df, gram_dict, source_prob, source_counts, total_grafs):\n",
    "    \n",
    "    \"\"\" \n",
    "    Calculates Information Gain for a given n-gram.\n",
    "    \"\"\"\n",
    "    \n",
    "    # p_gram is probability of seeing this gram across all paragraphs\n",
    "    # p_not_gram is 1 - p_gram\n",
    "    p_gram = gram_df/total_grafs\n",
    "    p_not_gram = 1 - p_gram\n",
    "    \n",
    "    # p(s|gram) is the probability of the source s given that we observe the gram\n",
    "    p_source_given_gram = [gram_dict[s]/gram_df for s in ['wd', 'ec', 'ny', 'ew']]\n",
    "    # If any of the probabilities are 0, change them to 1 so np.log(x) == 0\n",
    "    p_source_given_gram = [p if p!=0 else 1 for p in p_source_given_gram]\n",
    "    \n",
    "    # Î£(p(s|g) * log(p(s|g)))\n",
    "    s_given_g_entropy = sum([p * np.log(p) for p in p_source_given_gram])\n",
    "    \n",
    "    # p(s|not gram)\n",
    "    p_s_given_not_gram = [(source_counts[s] - gram_dict[s])/(total_grafs - gram_df) for s in ['wd', 'ec', 'ny', 'ew']]\n",
    "    # Change 0's to 1's so np.log(x) == 0\n",
    "    p_s_given_not_gram = [p if p!=0 else 1 for p in p_s_given_not_gram]\n",
    "    \n",
    "    # Î£(p(s|not g) * log(p(s|not g)))\n",
    "    p_s_given_not_g_entropy = sum([p * np.log(p) for p in p_s_given_not_gram])\n",
    "    \n",
    "    # Î£(p(s) * log(p(s))) \n",
    "    source_entropy = sum([(source_prob[s] * np.log(source_prob[s])) for s in ['wd', 'ec', 'ny', 'ew']])\n",
    "    \n",
    "    return (-source_entropy + (p_gram * s_given_g_entropy) + (p_not_gram * p_s_given_not_g_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Gain\n",
    "\n",
    "gram_ig = {gram: calc_ig(doc_freq[gram], all_gram_dicts[gram],\n",
    "                         source_prob, source_counts, total_grafs) for gram in set_grams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by IG value\n",
    "sorted_ig = [(term, gram_ig[term]) for term in sorted(gram_ig,\n",
    "                                                      key=gram_ig.get,\n",
    "                                                      reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of the proportional frequency of each n-gram in each publication, among\n",
    "# the top 20 n-grams by information gain\n",
    "\n",
    "top_ig_grams = sorted_ig[0:25]\n",
    "\n",
    "gram = [g for g, ig in top_ig_grams]\n",
    "gain = [ig for g, ig in top_ig_grams]\n",
    "ec_prop = np.round([ec_counter[g]/source_counts['ec']*100 for g, ig in top_ig_grams], 2)\n",
    "ny_prop = np.round([ny_counter[g]/source_counts['ny']*100 for g, ig in top_ig_grams], 2)\n",
    "wd_prop = np.round([wd_counter[g]/source_counts['wd']*100 for g, ig in top_ig_grams], 2)\n",
    "ew_prop = np.round([ew_counter[g]/source_counts['ew']*100 for g, ig in top_ig_grams], 2)\n",
    "\n",
    "top_grams_df = pd.DataFrame(list(zip(gram, gain, ec_prop, ny_prop, wd_prop, ew_prop)),\n",
    "                            columns=['gram', 'gain', 'ec_prop', 'ny_prop', 'wd_prop', 'ew_prop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram</th>\n",
       "      <th>gain</th>\n",
       "      <th>ec_prop</th>\n",
       "      <th>ny_prop</th>\n",
       "      <th>wd_prop</th>\n",
       "      <th>ew_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(., '')</td>\n",
       "      <td>0.081072</td>\n",
       "      <td>7.86</td>\n",
       "      <td>54.28</td>\n",
       "      <td>17.97</td>\n",
       "      <td>37.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Qx, .)</td>\n",
       "      <td>0.078511</td>\n",
       "      <td>7.85</td>\n",
       "      <td>53.45</td>\n",
       "      <td>18.00</td>\n",
       "      <td>37.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Qx, ., '')</td>\n",
       "      <td>0.078280</td>\n",
       "      <td>7.85</td>\n",
       "      <td>53.33</td>\n",
       "      <td>17.92</td>\n",
       "      <td>36.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(``, Qx, .)</td>\n",
       "      <td>0.077460</td>\n",
       "      <td>7.83</td>\n",
       "      <td>53.02</td>\n",
       "      <td>17.92</td>\n",
       "      <td>36.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(PRP, VBD)</td>\n",
       "      <td>0.064996</td>\n",
       "      <td>26.50</td>\n",
       "      <td>68.61</td>\n",
       "      <td>40.19</td>\n",
       "      <td>27.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>``</td>\n",
       "      <td>0.062612</td>\n",
       "      <td>37.78</td>\n",
       "      <td>74.82</td>\n",
       "      <td>31.74</td>\n",
       "      <td>47.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>''</td>\n",
       "      <td>0.062390</td>\n",
       "      <td>37.89</td>\n",
       "      <td>75.03</td>\n",
       "      <td>32.12</td>\n",
       "      <td>48.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qx</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>36.73</td>\n",
       "      <td>73.78</td>\n",
       "      <td>31.52</td>\n",
       "      <td>46.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(``, Qx)</td>\n",
       "      <td>0.060564</td>\n",
       "      <td>36.60</td>\n",
       "      <td>73.47</td>\n",
       "      <td>31.26</td>\n",
       "      <td>46.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(,, '')</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>9.51</td>\n",
       "      <td>49.67</td>\n",
       "      <td>19.09</td>\n",
       "      <td>33.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('', PRP, VBD)</td>\n",
       "      <td>0.058705</td>\n",
       "      <td>0.90</td>\n",
       "      <td>22.69</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(Qx, ,)</td>\n",
       "      <td>0.058384</td>\n",
       "      <td>9.51</td>\n",
       "      <td>49.32</td>\n",
       "      <td>19.04</td>\n",
       "      <td>32.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Qx, ,, '')</td>\n",
       "      <td>0.058384</td>\n",
       "      <td>9.51</td>\n",
       "      <td>49.32</td>\n",
       "      <td>19.04</td>\n",
       "      <td>32.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(``, Qx, ,)</td>\n",
       "      <td>0.057964</td>\n",
       "      <td>9.44</td>\n",
       "      <td>48.95</td>\n",
       "      <td>18.80</td>\n",
       "      <td>32.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(,, ADP)</td>\n",
       "      <td>0.048217</td>\n",
       "      <td>42.24</td>\n",
       "      <td>61.88</td>\n",
       "      <td>32.47</td>\n",
       "      <td>22.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(NOUN, VBD)</td>\n",
       "      <td>0.046093</td>\n",
       "      <td>46.52</td>\n",
       "      <td>76.07</td>\n",
       "      <td>48.34</td>\n",
       "      <td>38.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>VBD</td>\n",
       "      <td>0.045551</td>\n",
       "      <td>71.66</td>\n",
       "      <td>93.07</td>\n",
       "      <td>68.56</td>\n",
       "      <td>62.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(VBD, .)</td>\n",
       "      <td>0.045360</td>\n",
       "      <td>4.92</td>\n",
       "      <td>26.98</td>\n",
       "      <td>6.04</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(VBD, ,)</td>\n",
       "      <td>0.041181</td>\n",
       "      <td>4.27</td>\n",
       "      <td>25.36</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(VBD, ., ``)</td>\n",
       "      <td>0.040485</td>\n",
       "      <td>0.29</td>\n",
       "      <td>14.12</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(VBD, VERB)</td>\n",
       "      <td>0.039762</td>\n",
       "      <td>21.37</td>\n",
       "      <td>43.76</td>\n",
       "      <td>19.83</td>\n",
       "      <td>12.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>('', NOUN, VBD)</td>\n",
       "      <td>0.038516</td>\n",
       "      <td>0.24</td>\n",
       "      <td>14.88</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(., ``)</td>\n",
       "      <td>0.037779</td>\n",
       "      <td>10.50</td>\n",
       "      <td>39.85</td>\n",
       "      <td>17.14</td>\n",
       "      <td>33.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>('', PRP)</td>\n",
       "      <td>0.037386</td>\n",
       "      <td>2.90</td>\n",
       "      <td>27.78</td>\n",
       "      <td>8.98</td>\n",
       "      <td>12.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(,, VBD)</td>\n",
       "      <td>0.036660</td>\n",
       "      <td>18.02</td>\n",
       "      <td>32.40</td>\n",
       "      <td>10.72</td>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               gram      gain  ec_prop  ny_prop  wd_prop  ew_prop\n",
       "0           (., '')  0.081072     7.86    54.28    17.97    37.31\n",
       "1           (Qx, .)  0.078511     7.85    53.45    18.00    37.05\n",
       "2       (Qx, ., '')  0.078280     7.85    53.33    17.92    36.99\n",
       "3       (``, Qx, .)  0.077460     7.83    53.02    17.92    36.97\n",
       "4        (PRP, VBD)  0.064996    26.50    68.61    40.19    27.20\n",
       "5                ``  0.062612    37.78    74.82    31.74    47.65\n",
       "6                ''  0.062390    37.89    75.03    32.12    48.02\n",
       "7                Qx  0.060900    36.73    73.78    31.52    46.62\n",
       "8          (``, Qx)  0.060564    36.60    73.47    31.26    46.50\n",
       "9           (,, '')  0.059261     9.51    49.67    19.09    33.06\n",
       "10   ('', PRP, VBD)  0.058705     0.90    22.69     2.29     1.68\n",
       "11          (Qx, ,)  0.058384     9.51    49.32    19.04    32.94\n",
       "12      (Qx, ,, '')  0.058384     9.51    49.32    19.04    32.94\n",
       "13      (``, Qx, ,)  0.057964     9.44    48.95    18.80    32.85\n",
       "14         (,, ADP)  0.048217    42.24    61.88    32.47    22.57\n",
       "15      (NOUN, VBD)  0.046093    46.52    76.07    48.34    38.79\n",
       "16              VBD  0.045551    71.66    93.07    68.56    62.64\n",
       "17         (VBD, .)  0.045360     4.92    26.98     6.04     3.95\n",
       "18         (VBD, ,)  0.041181     4.27    25.36     7.00     3.60\n",
       "19     (VBD, ., ``)  0.040485     0.29    14.12     0.67     0.85\n",
       "20      (VBD, VERB)  0.039762    21.37    43.76    19.83    12.03\n",
       "21  ('', NOUN, VBD)  0.038516     0.24    14.88     1.44     1.34\n",
       "22          (., ``)  0.037779    10.50    39.85    17.14    33.00\n",
       "23        ('', PRP)  0.037386     2.90    27.78     8.98    12.65\n",
       "24         (,, VBD)  0.036660    18.02    32.40    10.72     6.40"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_grams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually select tags based on above table so we don't get repeats/multicollinearity\n",
    "\n",
    "selected_tags = ((\".\", \"''\"), # quote that ends a sentence (American English)\n",
    "                 (\"PRP\", \"VBD\"), # personal pronoun + past tense verb\n",
    "                 '``', # any quote start\n",
    "                 (\",\", \"''\"), # quote that ends with a comma (American English)\n",
    "                 (\"''\", \"PRP\", \"VBD\"), # end of a quote followed by a personal pronoun + past tense verb\n",
    "                 (\",\", \"ADP\"), # comma followed by a preposition or subordinating conjunction\n",
    "                 (\"NOUN\", \"VBD\"), # noun + past tense verb\n",
    "                 \"VBD\", # past tense verb\n",
    "                 (\"VBD\", \".\"), # past tense verb that ends sentence????\n",
    "                 (\"VBD\", \",\"), # past tense verb followed by comma?????\n",
    "                 (\"VBD\", \".\", \"``\"), # past tense verb that ends sentence, followed by beginning of quote???\n",
    "                 (\"VBD\", \"VERB\"), # past tense verb + another verb\n",
    "                 (\"''\", \"NOUN\", \"VBD\"), # end of a quote followed by noun + past tense verb\n",
    "                 (\".\", \"``\"), # quote starts at beginning of sentence\n",
    "                 (\"''\", \"PRP\"), # end quote + personal pronoun\n",
    "                 (\",\", \"VBD\") # past tense verb following a comma\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add POS Tag/Punctuation Features In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features based on selected_grams\n",
    "# Each feature is the count of the gram, normalized by the # of unigrams/bigrams/trigrams\n",
    "\n",
    "for gram in selected_tags:\n",
    "    \n",
    "    if isinstance(gram, str): # Unigrams\n",
    "        col = 'pos_tags'\n",
    "    elif isinstance(gram, tuple) and len(gram)==2:\n",
    "        col = 'tag_bigrams'\n",
    "    elif isinstance(gram, tuple) and len(gram)==3:\n",
    "        col = 'tag_trigrams'\n",
    "\n",
    "    final_df_filtered[gram] = final_df_filtered[col].apply(lambda tags: tags.count(gram)/len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(., '')</th>\n",
       "      <th>(PRP, VBD)</th>\n",
       "      <th>``</th>\n",
       "      <th>(,, '')</th>\n",
       "      <th>('', PRP, VBD)</th>\n",
       "      <th>(,, ADP)</th>\n",
       "      <th>(NOUN, VBD)</th>\n",
       "      <th>VBD</th>\n",
       "      <th>(VBD, .)</th>\n",
       "      <th>(VBD, ,)</th>\n",
       "      <th>(VBD, ., ``)</th>\n",
       "      <th>(VBD, VERB)</th>\n",
       "      <th>('', NOUN, VBD)</th>\n",
       "      <th>(., ``)</th>\n",
       "      <th>('', PRP)</th>\n",
       "      <th>(,, VBD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (., '')  (PRP, VBD)        ``  (,, '')  ('', PRP, VBD)  (,, ADP)  \\\n",
       "0      0.0    0.000000  0.000000      0.0             0.0  0.013333   \n",
       "1      0.0    0.000000  0.000000      0.0             0.0  0.013514   \n",
       "2      0.0    0.000000  0.000000      0.0             0.0  0.012346   \n",
       "3      0.0    0.009346  0.000000      0.0             0.0  0.000000   \n",
       "4      0.0    0.000000  0.013889      0.0             0.0  0.000000   \n",
       "\n",
       "   (NOUN, VBD)       VBD  (VBD, .)  (VBD, ,)  (VBD, ., ``)  (VBD, VERB)  \\\n",
       "0     0.013333  0.013158       0.0       0.0           0.0          0.0   \n",
       "1     0.000000  0.013333       0.0       0.0           0.0          0.0   \n",
       "2     0.000000  0.000000       0.0       0.0           0.0          0.0   \n",
       "3     0.000000  0.018519       0.0       0.0           0.0          0.0   \n",
       "4     0.000000  0.000000       0.0       0.0           0.0          0.0   \n",
       "\n",
       "   ('', NOUN, VBD)  (., ``)  ('', PRP)  (,, VBD)  \n",
       "0              0.0      0.0        0.0       0.0  \n",
       "1              0.0      0.0        0.0       0.0  \n",
       "2              0.0      0.0        0.0       0.0  \n",
       "3              0.0      0.0        0.0       0.0  \n",
       "4              0.0      0.0        0.0       0.0  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_filtered[list(selected_tags)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Dataframe for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>source</th>\n",
       "      <th>Content_Preprocessed</th>\n",
       "      <th>Content_NoQuotes</th>\n",
       "      <th>readability_SMOG</th>\n",
       "      <th>Sentence_Tokens</th>\n",
       "      <th>Num_Sentences</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>sd_sent_len</th>\n",
       "      <th>norm_stop_freq</th>\n",
       "      <th>...</th>\n",
       "      <th>(,, ADP)</th>\n",
       "      <th>(NOUN, VBD)</th>\n",
       "      <th>VBD</th>\n",
       "      <th>(VBD, .)</th>\n",
       "      <th>(VBD, ,)</th>\n",
       "      <th>(VBD, ., ``)</th>\n",
       "      <th>(VBD, VERB)</th>\n",
       "      <th>('', NOUN, VBD)</th>\n",
       "      <th>(., ``)</th>\n",
       "      <th>('', PRP)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>economist</td>\n",
       "      <td>DOWN the Euphrates river, halfway between Deir...</td>\n",
       "      <td>DOWN the Euphrates river, halfway between Deir...</td>\n",
       "      <td>11.2</td>\n",
       "      <td>[DOWN the Euphrates river, halfway between Dei...</td>\n",
       "      <td>3</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>11.841546</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>economist</td>\n",
       "      <td>Never have America and its allies had such a h...</td>\n",
       "      <td>Never have America and its allies had such a h...</td>\n",
       "      <td>10.7</td>\n",
       "      <td>[Never have America and its allies had such a ...</td>\n",
       "      <td>4</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.391165</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>economist</td>\n",
       "      <td>But like their Parthian forebears, Iran and it...</td>\n",
       "      <td>But like their Parthian forebears, Iran and it...</td>\n",
       "      <td>5.7</td>\n",
       "      <td>[But like their Parthian forebears, Iran and i...</td>\n",
       "      <td>5</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>5.844656</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>economist</td>\n",
       "      <td>Iran's gains are even more striking elsewhere....</td>\n",
       "      <td>Iran's gains are even more striking elsewhere....</td>\n",
       "      <td>11.6</td>\n",
       "      <td>[Iran's gains are even more striking elsewhere...</td>\n",
       "      <td>5</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>7.626270</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>economist</td>\n",
       "      <td>Farther south, America's hopes of stemming Ira...</td>\n",
       "      <td>Farther south, America's hopes of stemming Ira...</td>\n",
       "      <td>13.6</td>\n",
       "      <td>[Farther south, America's hopes of stemming Ir...</td>\n",
       "      <td>3</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>7.408704</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     source                               Content_Preprocessed  \\\n",
       "0      0  economist  DOWN the Euphrates river, halfway between Deir...   \n",
       "1      1  economist  Never have America and its allies had such a h...   \n",
       "2      2  economist  But like their Parthian forebears, Iran and it...   \n",
       "3      3  economist  Iran's gains are even more striking elsewhere....   \n",
       "4      4  economist  Farther south, America's hopes of stemming Ira...   \n",
       "\n",
       "                                    Content_NoQuotes  readability_SMOG  \\\n",
       "0  DOWN the Euphrates river, halfway between Deir...              11.2   \n",
       "1  Never have America and its allies had such a h...              10.7   \n",
       "2  But like their Parthian forebears, Iran and it...               5.7   \n",
       "3  Iran's gains are even more striking elsewhere....              11.6   \n",
       "4  Farther south, America's hopes of stemming Ira...              13.6   \n",
       "\n",
       "                                     Sentence_Tokens  Num_Sentences  \\\n",
       "0  [DOWN the Euphrates river, halfway between Dei...              3   \n",
       "1  [Never have America and its allies had such a ...              4   \n",
       "2  [But like their Parthian forebears, Iran and i...              5   \n",
       "3  [Iran's gains are even more striking elsewhere...              5   \n",
       "4  [Farther south, America's hopes of stemming Ir...              3   \n",
       "\n",
       "   avg_sent_len  sd_sent_len  norm_stop_freq    ...      (,, ADP)  \\\n",
       "0     20.666667    11.841546        0.421875    ...      0.013333   \n",
       "1     17.000000     3.391165        0.500000    ...      0.013514   \n",
       "2     13.800000     5.844656        0.422535    ...      0.012346   \n",
       "3     17.800000     7.626270        0.422222    ...      0.000000   \n",
       "4     18.666667     7.408704        0.362069    ...      0.000000   \n",
       "\n",
       "   (NOUN, VBD)       VBD (VBD, .) (VBD, ,)  (VBD, ., ``)  (VBD, VERB)  \\\n",
       "0     0.013333  0.013158      0.0      0.0           0.0          0.0   \n",
       "1     0.000000  0.013333      0.0      0.0           0.0          0.0   \n",
       "2     0.000000  0.000000      0.0      0.0           0.0          0.0   \n",
       "3     0.000000  0.018519      0.0      0.0           0.0          0.0   \n",
       "4     0.000000  0.000000      0.0      0.0           0.0          0.0   \n",
       "\n",
       "   ('', NOUN, VBD)  (., ``)  ('', PRP)  \n",
       "0              0.0      0.0        0.0  \n",
       "1              0.0      0.0        0.0  \n",
       "2              0.0      0.0        0.0  \n",
       "3              0.0      0.0        0.0  \n",
       "4              0.0      0.0        0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['index', 'source', 'Content_Preprocessed', 'Content_NoQuotes',\n",
       "       'readability_SMOG', 'Sentence_Tokens', 'Num_Sentences',\n",
       "       'avg_sent_len', 'sd_sent_len', 'norm_stop_freq', 'norm_punct_freq',\n",
       "       'norm_funct_freq', 'pos_tags', 'tag_bigrams', 'tag_trigrams',\n",
       "       (',', 'VBD'), ('.', \"''\"), ('PRP', 'VBD'), '``', (',', \"''\"),\n",
       "       (\"''\", 'PRP', 'VBD'), (',', 'ADP'), ('NOUN', 'VBD'), 'VBD',\n",
       "       ('VBD', '.'), ('VBD', ','), ('VBD', '.', '``'), ('VBD', 'VERB'),\n",
       "       (\"''\", 'NOUN', 'VBD'), ('.', '``'), (\"''\", 'PRP')], dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_filtered.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_features = ['source','readability_SMOG','avg_sent_len', 'Num_Sentences',\n",
    "                                 'sd_sent_len','norm_stop_freq','norm_punct_freq','norm_funct_freq']\n",
    "\n",
    "df_features = final_df_filtered[list(selected_tags)+og_features]\n",
    "\n",
    "# NOTE: Descriptive statistics for SMOG show more variation than Flesch, so dropped Flesch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(., '')</th>\n",
       "      <th>(PRP, VBD)</th>\n",
       "      <th>``</th>\n",
       "      <th>(,, '')</th>\n",
       "      <th>('', PRP, VBD)</th>\n",
       "      <th>(,, ADP)</th>\n",
       "      <th>(NOUN, VBD)</th>\n",
       "      <th>VBD</th>\n",
       "      <th>(VBD, .)</th>\n",
       "      <th>(VBD, ,)</th>\n",
       "      <th>...</th>\n",
       "      <th>('', PRP)</th>\n",
       "      <th>(,, VBD)</th>\n",
       "      <th>source</th>\n",
       "      <th>readability_SMOG</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>Num_Sentences</th>\n",
       "      <th>sd_sent_len</th>\n",
       "      <th>norm_stop_freq</th>\n",
       "      <th>norm_punct_freq</th>\n",
       "      <th>norm_funct_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>economist</td>\n",
       "      <td>11.2</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>11.841546</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>economist</td>\n",
       "      <td>10.7</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3.391165</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>economist</td>\n",
       "      <td>5.7</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>5.844656</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.436620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>economist</td>\n",
       "      <td>11.6</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>7.626270</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>economist</td>\n",
       "      <td>13.6</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>7.408704</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.344828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (., '')  (PRP, VBD)        ``  (,, '')  ('', PRP, VBD)  (,, ADP)  \\\n",
       "0      0.0    0.000000  0.000000      0.0             0.0  0.013333   \n",
       "1      0.0    0.000000  0.000000      0.0             0.0  0.013514   \n",
       "2      0.0    0.000000  0.000000      0.0             0.0  0.012346   \n",
       "3      0.0    0.009346  0.000000      0.0             0.0  0.000000   \n",
       "4      0.0    0.000000  0.013889      0.0             0.0  0.000000   \n",
       "\n",
       "   (NOUN, VBD)       VBD  (VBD, .)  (VBD, ,)       ...         ('', PRP)  \\\n",
       "0     0.013333  0.013158       0.0       0.0       ...               0.0   \n",
       "1     0.000000  0.013333       0.0       0.0       ...               0.0   \n",
       "2     0.000000  0.000000       0.0       0.0       ...               0.0   \n",
       "3     0.000000  0.018519       0.0       0.0       ...               0.0   \n",
       "4     0.000000  0.000000       0.0       0.0       ...               0.0   \n",
       "\n",
       "   (,, VBD)     source  readability_SMOG  avg_sent_len  Num_Sentences  \\\n",
       "0       0.0  economist              11.2     20.666667              3   \n",
       "1       0.0  economist              10.7     17.000000              4   \n",
       "2       0.0  economist               5.7     13.800000              5   \n",
       "3       0.0  economist              11.6     17.800000              5   \n",
       "4       0.0  economist              13.6     18.666667              3   \n",
       "\n",
       "  sd_sent_len  norm_stop_freq  norm_punct_freq  norm_funct_freq  \n",
       "0   11.841546        0.421875         0.234375         0.390625  \n",
       "1    3.391165        0.500000         0.132353         0.470588  \n",
       "2    5.844656        0.422535         0.197183         0.436620  \n",
       "3    7.626270        0.422222         0.255556         0.411111  \n",
       "4    7.408704        0.362069         0.327586         0.344828  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle for modeling\n",
    "\n",
    "with open('pickles/features_04_30.pkl', 'wb') as f:\n",
    "    pickle.dump(df_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SVM ... (move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> features, y -> label\n",
    "X=df_features[['readability_f','readability_SMOG','avg_sent_len','sd_sent_len','norm_stop_freq','norm_punct_freq','norm_funct_freq']]\n",
    "y=df_features.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing X, y into train and test data \n",
    "#We use stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "svm_predictions = svm_model_linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49770107703\n"
     ]
    }
   ],
   "source": [
    "accuracy = svm_model_linear.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1475  258  760  802]\n",
      " [ 236 1437 1245  806]\n",
      " [ 517  436 3357  424]\n",
      " [ 946  696  849 1633]]\n"
     ]
    }
   ],
   "source": [
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, svm_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_model_radial = SVC(kernel = 'rbf', C = 1).fit(X_train, y_train)\n",
    "svm_predictions = svm_model_radial.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = svm_model_radial.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
